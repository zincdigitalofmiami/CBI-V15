# Augment Workspace Instructions for CBI-V15

## Project Overview
CBI-V15 is a **soybean oil futures (ZL) forecasting system** using:
- **AutoGluon 1.4** (TabularPredictor + TimeSeriesPredictor with foundation models)
- **DuckDB/MotherDuck** for data storage (NO BigQuery, NO Dataform)
- **AnoFox SQL macros** for feature engineering (1,428+ lines of SQL)
- **Mac M4** for local training (100% offline-capable)
- **Trigger.dev** for orchestration (NO Airflow, NO Prefect)

## CRITICAL: Always Read First (in order)

Before starting ANY task:

1. **`docs/architecture/MASTER_PLAN.md`** ‚Äî V15.1 architecture source of truth (249 lines)
2. **`AGENTS.md`** ‚Äî AI assistant guardrails, Big 8 buckets, naming conventions (128 lines)
3. **`database/README.md`** ‚Äî 8-schema layout, feature boundaries (54 lines)
4. **`/Users/zincdigital/.cursor/plans/autogluon_hybrid_implementation_c2287cb0.plan.md`** ‚Äî Current implementation plan with pending tasks

## Technology Stack (DO NOT DEVIATE)

| Component | Technology | Path | Status |
|-----------|-----------|------|--------|
| **Database (Source of Truth)** | **MotherDuck Cloud** | `md:cbi_v15` | ‚úÖ Primary (permanent) |
| **Training Workspace** | DuckDB (local Mac) | `data/duckdb/cbi_v15.duckdb` | ‚úÖ Persistent (NOT deleted) |
| **ML Framework** | AutoGluon 1.4 | `src/training/autogluon/` | üöß In Progress |
| **Feature Engineering** | SQL macros (AnoFox) | `database/macros/` | ‚úÖ Active |
| **Orchestration** | Trigger.dev | `trigger/` | ‚úÖ Active |
| **Training** | Mac M4 (local CPU) | Local only | ‚úÖ Active |
| **Dashboard** | Next.js/Vercel | `dashboard/` | ‚úÖ Active |
| **Legacy (REMOVED)** | BigQuery/Dataform/GCP | `_archive/bigquery_legacy_*` | ‚ùå Deprecated |

## Data Sources (38 Futures Symbols)

**Only use data sources listed in `docs/data_sources/DATA_SOURCES_MASTER.md`**

### Verified Sources:
- **Databento**: 38 futures (ZL/ZS/ZM/CL/HO/RB/HG/6L/DX/etc.) - OHLCV daily
- **FRED**: 24+ macro indicators (FEDFUNDS, DGS10, VIXCLS, NFCI, STLFSI4, etc.)
- **EPA**: RIN prices D3/D4/D5/D6 (weekly, FREE) ‚ö†Ô∏è Pending ingestion
- **EIA**: Petroleum products (ULSD wholesale, gasoline, diesel)
- **USDA FAS**: Export sales, WASDE reports
- **CFTC**: COT positioning data (weekly) ‚ö†Ô∏è Pending ingestion
- **ScrapeCreators**: Trump posts + 8 news buckets
- **Farm Policy News**: Real-time China/tariff policy ‚ö†Ô∏è Pending scraper
- **farmdoc Daily**: Academic ag economics (Scott Irwin RIN models) ‚ö†Ô∏è Pending scraper
- **ProFarmer**: Weather, basis, barge rates ‚ö†Ô∏è Pending integration
- **NOAA/INMET/Argentina SMN**: Weather data

### Invalid Symbols (DO NOT USE):
- ‚ùå OJ (Orange Juice) - NOT available via CME (trades on ICE)
- ‚ùå UL (ULSD) - Does NOT exist as separate symbol; use **HO**
- ‚ùå AL (Aluminum) - NOT available via CME (trades on LME)
- ‚ùå TY - Use **ZN** instead (TY is legacy floor symbol)

## Big 8 Buckets (Focus Overlays)

All models must cover these 8 drivers (models see ALL features, buckets are focus overlays):

1. **Crush**: ZL/ZS/ZM spreads, board crush, oil share
2. **China**: HG-ZS correlation, export sales, trade policy
3. **FX**: DX, BRL (6L), CNY, MXN momentum/volatility
4. **Fed**: Fed funds, yield curve (T10Y2Y), NFCI, STLFSI4
5. **Tariff**: Trump sentiment, Farm Policy News, Section 301
6. **Biofuel**: EPA RIN D4/D5/D6, BOHO spread, biodiesel
7. **Energy**: CL/HO/RB, crack spreads, CL-ZL correlation
8. **Volatility**: VIX, realized vol, stress indices

## Naming Conventions (MANDATORY)

### Feature Naming Pattern
`{source}_{symbol}_{indicator}_{param}_{transform}`

**Examples:**
- ‚úÖ `databento_zl_close`, `fred_VIXCLS_close`, `epa_rin_d4_price`
- ‚úÖ `volatility_zl_21d`, `volume_zl_21d`, `cftc_zl_managed_money_net_pct`
- ‚ùå `vol_zl_21d` (ambiguous - volatility or volume?)
- ‚ùå `volat_regime` (inconsistent)

### Critical Distinction: Volatility vs Volume
- **Volatility** (price variance): `volatility_*` (e.g., `volatility_zl_21d`)
- **Volume** (trading activity): `volume_*` (e.g., `volume_zl_21d`)
- **NEVER USE**: `vol_*` alone (ambiguous)

### Source Prefixes
| Prefix | Source | Example |
|--------|--------|---------|
| `databento_*` | Futures OHLCV | `databento_zl_close` |
| `fred_*` | FRED economic | `fred_FEDFUNDS`, `fred_DGS10` |
| `epa_*` | EPA RIN prices | `epa_rin_d4_price` |
| `eia_*` | EIA biofuels/petroleum | `eia_biodiesel_prod` |
| `usda_*` | USDA export/WASDE | `usda_export_soybeans_weekly` |
| `cftc_*` | CFTC COT | `cftc_zl_net_noncomm` |
| `scrc_*` | ScrapeCreators | `scrc_biofuel_policy_sentiment` |
| `weather_*` | NOAA/INMET/SMN | `weather_brazil_precip` |
| `volatility_*` | Calculated volatility | `volatility_vix_close` |
| `volume_*` | Trading volume | `volume_zl_21d` |

## File Organization

```
CBI-V15/
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ definitions/         # DDL files (8 schemas)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00_init/        # Schema creation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_raw/         # Raw data tables
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_staging/     # Cleaned data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_features/    # Engineered features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_training/    # ML training matrices
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_assertions/  # Data quality checks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 06_api/         # Dashboard views
‚îÇ   ‚îú‚îÄ‚îÄ macros/             # AnoFox SQL feature macros (1,428 lines)
‚îÇ   ‚îî‚îÄ‚îÄ views/              # Internal analysis views
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/          # Data collectors (Databento, FRED, EIA, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autogluon/      # AutoGluon TabularPredictor + TimeSeriesPredictor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ baselines/      # Legacy LightGBM/CatBoost/XGBoost
‚îÇ   ‚îú‚îÄ‚îÄ engines/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anofox/         # AnoFox SQL macro bridge
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ tsci/           # TSci LLM agents (optional)
‚îú‚îÄ‚îÄ trigger/                # Trigger.dev orchestration jobs
‚îú‚îÄ‚îÄ scripts/                # Operational utilities (setup, validation)
‚îú‚îÄ‚îÄ config/                 # YAML/JSON configuration
‚îú‚îÄ‚îÄ dashboard/              # Next.js/Vercel dashboard
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture/       # MASTER_PLAN.md, design docs
‚îÇ   ‚îú‚îÄ‚îÄ data_sources/       # DATA_SOURCES_MASTER.md
‚îÇ   ‚îú‚îÄ‚îÄ features/           # Feature engineering docs
‚îÇ   ‚îî‚îÄ‚îÄ reference/          # BEST_PRACTICES.md
‚îî‚îÄ‚îÄ _archive/               # Legacy BigQuery/Dataform code (deprecated)
```

## Hard Stop Rules (CRITICAL)

1. **NO FAKE DATA**: Never use placeholders, synthetic data, or mock values
2. **ALWAYS CHECK BEFORE CREATING**: Verify tables/files exist before modifying
3. **ALWAYS AUDIT AFTER WORK**: Run data quality checks after changes
4. **NO COSTLY RESOURCES**: No paid services without approval (>$5/month)
5. **API KEYS**: macOS Keychain or `.env` files, NEVER hardcoded
6. **DuckDB/MotherDuck ONLY**: No BigQuery, No Dataform, No GCP
7. **SQL FEATURES FIRST**: All feature engineering in SQL macros, NOT Python loops
8. **MAC TRAINING ONLY**: All training on Mac M4, no cloud training
9. **CLOSE PRICES ONLY**: Do NOT reference Open/High/Low/Volume for price features
10. **NO NEW MARKDOWN FILES**: Update existing docs in-place unless permanent

## How to Build Implementation Plans

### Phase Structure (REQUIRED)

```markdown
## Phase 0: Critical Bugs (MUST complete first)
- Fix Databento column: date ‚Üí as_of_date in collect_daily.py
- Fix FRED table: fred_observations ‚Üí fred_economic in big8_bucket_features.sql
- Fix EIA table: eia_biofuels ‚Üí eia_petroleum in big8_bucket_features.sql
- Add missing FRED series: DFEDTARU, VIXCLS to collectors
- Create local DuckDB mirror and sync script

## Phase 1: Data Ingestion (after Phase 0)
- Create EPA RIN prices Trigger job (FREE data, July 2010-present)
- Remove mock data from USDA export sales, implement real API
- Create CFTC COT Trigger job
- Create FRED daily ingest Trigger job
- Create Farm Policy News scraper (farmpolicynews.illinois.edu)
- Create farmdoc Daily scraper (farmdocdaily.illinois.edu)

## Phase 2: AutoGluon Setup (after Phase 1)
- Add AutoGluon 1.4 to requirements.txt
- Create Mac M4 setup script (libomp fix for AutoGluon)
- Create TabularPredictor wrapper (quantile regression)
- Create TimeSeriesPredictor wrapper (Chronos-Bolt)
- Update engine_registry.py with AutoGluon models

## Phase 3: Bucket Specialists (after Phase 2)
- Create bucket_feature_selectors.yaml (Big 8 feature lists)
- Create bucket specialist training configs (8 buckets)
- Create train_all_buckets.py orchestrator

## Phase 4: Ensemble (after Phase 3)
- Create ensemble_tables.sql schema
- Create greedy_ensemble.py
- Update monte_carlo_sim.py for AutoGluon output

## Phase 5: Production Pipeline (after Phase 4)
- Create Trigger.dev feed jobs (main + 8 buckets)
- Create daily_zl_forecast.ts pipeline
```

### Task Requirements

Each task MUST specify:
- **Exact file path** (no placeholders)
- **Specific table/schema names** (verify in `database/definitions/`)
- **Data sources** (verify in `docs/data_sources/DATA_SOURCES_MASTER.md`)
- **Validation steps** (how to test it works)
- **Expected output** (schema, columns, row counts)

### Example Valid Task

```markdown
- [ ] Create EPA RIN prices ingestion
      File: src/ingestion/epa/collect_rin_prices.py
      Target: raw.epa_rin_prices (as_of_date, rin_type, price, volume)
      Source: https://www.epa.gov/fuels-registration-reporting-and-compliance-help/rin-trades-and-price-information
      Frequency: Weekly (updates every Friday)
      Validation: SELECT COUNT(*) FROM raw.epa_rin_prices WHERE rin_type = 'D4'
      Expected: ~700 rows (July 2010 - present, weekly data)
```

## Anti-Patterns to Avoid

### ‚ùå DO NOT DO THIS:
- Creating new markdown documentation files (update existing in-place)
- Hardcoding API keys in Python/TypeScript code
- Building features in Python loops (use SQL macros)
- Training models before data pipeline works
- Proposing BigQuery/Dataform/GCP resources
- Inventing new symbols not in the 38-symbol list
- Creating placeholders or mock data
- Skipping validation steps
- Using `vol_*` alone (ambiguous - volatility or volume?)
- Referencing TY, OJ, UL, AL symbols (don't exist or unavailable)

### ‚úÖ DO THIS INSTEAD:
- Update existing markdown files with new sections
- Use `.env` files or macOS Keychain for API keys
- Create SQL macros in `database/macros/` for features
- Fix Phase 0 bugs before adding new features
- Use DuckDB/MotherDuck exclusively
- Only use 38 documented futures symbols
- Use real data from verified APIs
- Include validation for every task
- Use `volatility_*` or `volume_*` (explicit)
- Use ZN (not TY), HO (not UL), skip OJ/AL

## Validation Commands

After making changes, run these checks:

```bash
# Sync MotherDuck ‚Üí Local (BEFORE training)
python scripts/sync_motherduck_to_local.py

# Database schema
python scripts/setup/execute_local_duckdb_schema.py
python scripts/setup/deploy_schema_to_motherduck.py

# System status
bash scripts/system_status.sh

# Data quality
python scripts/validation/data_quality_checks.py

# API keys
bash scripts/setup/verify_api_keys.sh

# Test ingestion (example)
python src/ingestion/databento/collect_daily.py --local
```

## Current Implementation Status

**In Progress**: AutoGluon Hybrid Implementation (Phase 0-5)

**Phase 0 Completed**:
- ‚úÖ Local DuckDB mirror created (`data/duckdb/cbi_v15.duckdb`)
- ‚úÖ Sync script created (`scripts/sync_motherduck_to_local.py`)
- ‚úÖ Architecture verified (ATTACH pattern with hybrid execution)

**Phase 0 Pending Tasks**:
- Fix Databento column name (date ‚Üí as_of_date)
- Fix FRED table reference (fred_observations ‚Üí fred_economic)
- Fix EIA table reference (eia_biofuels ‚Üí eia_petroleum)
- Add missing FRED series (DFEDTARU, VIXCLS)

**See**: `/Users/zincdigital/.cursor/plans/autogluon_hybrid_implementation_c2287cb0.plan.md` for complete task list

## MotherDuck ‚Üí Local Sync Architecture (VERIFIED)

**üéØ TWO-DATABASE ARCHITECTURE:**

- **MotherDuck Cloud** = Source of Truth (permanent, all production data)
- **Local Mac DuckDB** = Training Workspace (persistent, fast I/O for AutoGluon)

**Complete Data Flow**:
```
1. Trigger.dev jobs (cloud) ‚Üí MotherDuck (SOURCE OF TRUTH - permanent)
                                    ‚Üì
2. Before training: sync_motherduck_to_local.py ‚Üí Local Mac (TRAINING WORKSPACE - persistent)
                                    ‚Üì
3. AutoGluon training (Mac M4) ‚Üí Reads/writes local workspace (100-1000x faster)
                                    ‚Üì
4. Upload predictions ‚Üí MotherDuck (forecasts.zl_predictions)
                                    ‚Üì
5. Dashboard (Vercel) ‚Üí Queries MotherDuck directly
```

**Sync Pattern** (refresh local workspace with latest data):
```python
# 1. Connect to local DuckDB (persistent training workspace)
conn = duckdb.connect('data/duckdb/cbi_v15.duckdb')

# 2. ATTACH MotherDuck (SOURCE OF TRUTH) as 'md_source'
conn.execute("ATTACH 'md:cbi_v15?motherduck_token=TOKEN' AS md_source")

# 3. Refresh tables from cloud (updates local workspace with latest data)
conn.execute("""
    CREATE OR REPLACE TABLE raw.databento_ohlcv_daily AS 
    SELECT * FROM md_source.raw.databento_ohlcv_daily
""")
```

**Why This Architecture**:
- ‚úÖ **MotherDuck = Source of Truth** (permanent, cloud, accessible everywhere)
- ‚úÖ **Local Mac = Training Workspace** (persistent, fast local I/O for AutoGluon)
- ‚úÖ **Trigger.dev writes to MotherDuck** (source of truth)
- ‚úÖ **Sync refreshes local workspace** (before training to get latest data)
- ‚úÖ **Dashboard reads MotherDuck** (NOT local Mac)
- ‚úÖ **Local workspace persists** (NOT deleted after training)

**Critical Rules**:
1. **All ingestion ‚Üí MotherDuck directly** (Trigger.dev jobs)
2. **Sync to local before training** (refresh workspace with latest data)
3. **Local workspace is persistent** (NOT deleted - it's your training area)
4. **Upload predictions to MotherDuck** (source of truth)
5. **Dashboard queries MotherDuck** (NOT local Mac)

**Sources**:
- [MotherDuck Docs: Loading DuckDB Database](https://motherduck.com/docs/key-tasks/loading-data-into-motherduck/loading-duckdb-database/)
- [MotherDuck Docs: Attach Modes](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/attach-modes/)
- [DuckDB Docs: Multi-Database Transactions](https://duckdb.org/docs/stable/sql/statements/attach)
- [VLDB Paper: MotherDuck Hybrid Execution](https://vldb.org/cidrdb/papers/2024/p46-atwal.pdf)

## Questions to Ask Before Starting

1. **Does this task depend on Phase 0 bugs being fixed?** If yes, fix Phase 0 first.
2. **Does the data source exist in DATA_SOURCES_MASTER.md?** If no, STOP and ask user.
3. **Does the table/schema exist in database/definitions/?** If no, verify before creating.
4. **Is this using DuckDB/MotherDuck or BigQuery?** If BigQuery, STOP - wrong architecture.
5. **Is this feature in SQL or Python?** If Python loop, STOP - use SQL macro.
6. **Does the symbol exist in the 38-symbol list?** If no (OJ, UL, AL, TY), STOP.
7. **Is there a validation step?** If no, add one before proceeding.
8. **Does this follow naming conventions?** Check `volatility_*` vs `volume_*`.

## Summary

- **Read**: MASTER_PLAN.md, AGENTS.md, database/README.md FIRST
- **Stack**: DuckDB/MotherDuck + AutoGluon 1.4 + Mac M4 + Trigger.dev
- **No**: BigQuery, Dataform, GCP, cloud training, fake data
- **Naming**: `{source}_{symbol}_{indicator}_{param}`, explicit volatility/volume
- **Validate**: Every task must have validation steps
- **Phase 0**: Fix bugs before adding features
