#!/usr/bin/env python3
"""
Auto-generate bucket_feature_selectors.yaml from live database schema.

This script:
1. Connects to training.daily_ml_matrix_zl
2. Reads actual column names (79 columns)
3. Auto-assigns columns to buckets using keyword matching
4. Generates config/bucket_feature_selectors.yaml

This ensures 100% alignment between config and actual data.
"""

import duckdb
import yaml
import os
from dotenv import load_dotenv

load_dotenv()
DB_NAME = os.getenv("MOTHERDUCK_DB", "cbi_v15")
TOKEN = os.getenv("MOTHERDUCK_TOKEN")


def generate_config():
    print(f"üîå Connecting to md:{DB_NAME}...")
    con = duckdb.connect(f"md:{DB_NAME}?motherduck_token={TOKEN}")

    # 1. Get Actual Columns from the Training Table
    print("üîç Inspecting table schema...")
    try:
        # Fetch column names directly from the source of truth
        cols = con.execute("DESCRIBE training.daily_ml_matrix_zl").fetchall()
        all_cols = [c[0] for c in cols]
    except Exception as e:
        print(f"‚ùå Error reading table: {e}")
        return

    # Filter out non-features (targets, metadata, splits)
    exclude = {
        "as_of_date",
        "symbol",
        "split_type",
        "train_val_test_split",
        "training_weight",
        "updated_at",
        "target_price_1w",
        "target_price_1m",
        "target_price_3m",
        "target_price_6m",
        "target_ret_1w",
        "target_ret_1m",
        "target_ret_3m",
        "target_ret_6m",
        "target_ret_12m",
        "target_ret_1w_1",
        "target_ret_1m_1",
        "target_ret_3m_1",
        "target_ret_6m_1",
    }
    feature_cols = [c for c in all_cols if c not in exclude]

    print(f"‚úÖ Found {len(feature_cols)} valid features in the database.")
    print(f"üëÄ Sample: {feature_cols[:10]}...")

    # 2. Define Bucket Keywords (The Logic)
    # This maps raw column names to buckets based on substrings
    buckets = {
        "core_base": [
            "close",
            "rsi",
            "macd",
            "bb_",
            "atr",
            "stoch",
            "roc_",
            "momentum",
            "volume",
            "obv",
        ],
        "crush": [
            "crush",
            "oil_share",
            "board",
            "zs",
            "zm",
            "zl",
            "soybean",
            "meal",
        ],
        "china": [
            "china",
            "hg",
            "copper",
            "export",
            "pulse",
        ],
        "fx": [
            "dollar_index",
            "dx",
            "brl",
            "cny",
            "eur",
            "fx_",
        ],
        "fed": [
            "yield",
            "curve",
            "fed_",
            "rate",
        ],
        "tariff": [
            "tariff",
            "trade",
            "activity",
        ],
        "biofuel": [
            "rin",
            "biofuel",
            "boho",
            "biodiesel",
        ],
        "energy": [
            "crude",
            "energy",
            "cl",
            "ho",
            "rb",
            "crack",
        ],
        "volatility": [
            "vix",
            "volatility",
            "atr",
            "bb_width",
            "tr_pct",
        ],
    }

    # 3. Build the Config Dictionary
    config_out = {}

    for bucket, keywords in buckets.items():
        assigned = []
        for col in feature_cols:
            # Assign column if it matches any keyword for this bucket
            if any(k in col.lower() for k in keywords):
                assigned.append(col)

        # Deduplicate and sort
        config_out[bucket] = sorted(list(set(assigned)))

    # Fallback: Ensure core_base has essential columns
    essentials = ["close", "vix"]
    for ess in essentials:
        if ess in feature_cols and ess not in config_out["core_base"]:
            config_out["core_base"].append(ess)

    # 4. Add correlation features to relevant buckets
    corr_cols = [c for c in feature_cols if c.startswith("corr_")]
    for bucket in ["china", "fx", "energy", "crush"]:
        config_out[bucket].extend(corr_cols[:5])  # Add top 5 correlations
        config_out[bucket] = sorted(list(set(config_out[bucket])))

    # 5. Write to YAML
    output_path = "config/bucket_feature_selectors.yaml"
    with open(output_path, "w") as f:
        f.write("# AUTO-GENERATED CONFIG FROM LIVE DB SCHEMA\n")
        f.write("# Generated by scripts/generate_config_from_db.py\n")
        f.write("# This matches the RAW column names in training.daily_ml_matrix_zl\n")
        f.write(f"# Total features discovered: {len(feature_cols)}\n")
        f.write("# Date generated: " + str(os.popen("date").read().strip()) + "\n\n")
        yaml.dump(config_out, f, default_flow_style=False, sort_keys=False)

    print(f"\n‚ú® Generated {output_path} with {len(feature_cols)} mapped columns!")
    print("\nüìä Feature counts per bucket:")
    for bucket, features in config_out.items():
        print(f"   {bucket:15s}: {len(features):2d} features")

    print("\n   Please review the file to ensure the logic held up.")

    con.close()


if __name__ == "__main__":
    generate_config()



