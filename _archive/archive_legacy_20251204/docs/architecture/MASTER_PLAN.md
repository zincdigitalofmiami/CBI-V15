---
**âš ï¸ CRITICAL: NO FAKE DATA âš ï¸**
This project uses ONLY real, verified data sources. NO placeholders, NO synthetic data, NO fake values.
All data must come from authenticated APIs, official sources, or validated historical records.
---

**ğŸ“‹ BEST PRACTICES:** See `docs/reference/BEST_PRACTICES.md` for mandatory best practices including: no fake data, always check before creating, always audit after work, us-central1 only, no costly resources without approval, research best practices, research quant finance modeling.

# FRESH START MASTER PLAN
**Date:** November 18, 2025  
**Status:** Complete Rebuild - Robust Architecture  
**Purpose:** Single source of truth for new architecture with source prefixing, DataBento live futures, expanded FRED, and a canonical feature table (`master_features_2000_2025.parquet`) that mirrors to BigQuery for Ultimate Signal, Big 8, MAPE, and Sharpe dashboards.

---

> Migration reference: See `docs/plans/BIGQUERY_MIGRATION_PLAN.md` for the adopted BigQueryâ€‘centric data plane (ingest/orchestrate/serve), Macâ€‘centric model plane (train/score), and Vercel readâ€‘only strategy.

### Architecture Shift (Summary)
- BigQuery is the system of record and orchestrator (Scheduled Queries/Dataform) for curated surfaces and dashboard views.
- Mac M4 pulls Parquet bundles from GCS, trains/scores locally, and pushes forecasts/diagnostics back to BigQuery.
- 5â€‘minute batch ingestion to BigQuery; dashboard polls BQ views every 5 minutes.

## PHILOSOPHY

**"Fresh slate, robust architecture, source-prefixed columns, robust data collection"**

- **Architecture napkin (keep it simple):**
  - Flow: Sources â†’ BigQuery `raw.*` â†’ `staging.*` / `reference.*` â†’ `training.daily_ml_matrix` â†’ Mac (train/predict) â†’ BigQuery `forecasts.*` / `signals.*` â†’ Vercel/dashboard.
  - Ownership: Ingest = Python/Cloud jobs writing into `raw.*`; Transform = Python + BigQuery SQL jobs building `staging.*` / `features.*` / `training.*`; Train = Mac; Serve = BigQuery/Vercel.
  - Transformation bands (no Dataform): RAWâ†’STAGING (type, dedupe, basic QA) via Python+SQL; STAGING/RAWâ†’FEATURES (signals, wide pivots) in Python; FEATURES/RAWâ†’TRAINING/FORECASTS (frozen slices, thin tables) via Python loaders + SQL views.
  - Schedules (concept): Ingest pull â†’ Python/SQL transforms to staging/features/training â†’ Training freeze + forecasts after Mac push.
  - Names (concept level): `raw.*`, `raw_staging.*`, `staging.*`, `features.*`, `training.*`, `forecasts.*`, `signals.*`, `api.*`, `ops.*`, `reference.*`, `archive.*`.
  - Mental model: Ingest = Python/BigQuery job; Transform = Python feature builders + BigQuery views; Train = Mac job; Serve = BigQuery/Vercel.

- **Mac M4:** ALL training + ALL feature engineering (local, deterministic)
- **BigQuery:** Storage + Scheduling + Dashboard (NOT training)
- **External Drive:** PRIMARY data storage + Backup
- **Dual Storage:** Parquet (external drive) + BigQuery (mirror)
- **Staging + Quarantine:** No bad data reaches training
- **Declarative joins:** YAML spec with automated tests
- **QA gates:** Block between every phase

---

## FOR AI ASSISTANTS: CRITICAL CONTEXT

> **This section is essential reading for all AI assistants working on CBI-V15. It provides critical architecture context, guardrails, and workflow patterns.**

> For a concise, always-current contract of what assistants are and arenâ€™t allowed to do in this repo, see:  
> `docs/architecture/AI_ASSISTANT_RULES.md`.

### Current Architecture (November 17, 2025)
- Apple M4 Mac handles every training and inference task (TensorFlow Metal + PyTorch MPS + CPU tree libs).
- BigQuery = storage plus dashboard read layer only; no BigQuery ML, no AutoML jobs.
- Predictions generated locally, uploaded with `scripts/upload_predictions.py`, then read by the Vercel dashboard.
- The `vertex-ai/` directory is kept for reference. See `vertex-ai/LEGACY_MARKER.md`. Do **not** run anything in there.
- First-run models save to `Models/local/.../{model}` without version suffix. Pass an explicit `version` only when promoting a later retrain.
- **ZL = Soybean Oil Futures** (CBOT ticker) - NOT corn. Primary commodity for single-asset baseline.

### Architecture Pattern: Hybrid Python + BigQuery SQL
**Data Collection**: Python scripts â†’ External drive (`/Volumes/Satechi Hub/`) + BigQuery  
**Feature Engineering**: HYBRID approach (already in use):
  - BigQuery SQL: Correlations (CORR() OVER window), moving averages, regimes (existing in `advanced_feature_engineering.sql`)
  - Python: Complex sentiment, NLP, policy extraction, interactions (existing in `feature_calculations.py`)
  - **Both are used** - this is the production pattern

### Data Sources (Formalized November 2025)
1. **FRED**: 30+ economic series (interest rates, inflation, employment, GDP) via `collect_fred_comprehensive.py`
2. **DataBento**: Futures OHLCV (GLBX.MDP3) â€” primary authoritative feed (1m/1d)
4. **World Bank (Alternative Macro)**: Optional complementary macro series (growth, development, trade) via `scripts/ingest/collect_worldbank_alternative.py`. Used only where explicitly documented and never as a stealth replacement for FRED series.

### Primary Documents
- `docs/plans/MASTER_PLAN.md` (this document) â€“ Source of truth
- `docs/plans/TRAINING_PLAN.md` â€“ Training strategy and execution
- `docs/plans/BIGQUERY_MIGRATION_PLAN.md` â€“ BigQuery migration strategy
- `docs/PAGE_BUILDOUT_ROADMAP.md` â€“ Dashboard page specifications and buildout plan
- `docs/reports/costs/AI_MIGRATION_NIGHTMARE.md` â€“ âš ï¸ CRITICAL: AI created ~$250/month mistake. Read before creating any GCP resources.

### Reference Documents (Technical Specifications)
- `docs/reference/STRATEGY_SCENARIO_CARDS.md` â€“ Strategy page scenario cards specification
- `docs/reference/FIBONACCI_MATH.md` â€“ Fibonacci retracements and extensions (Databento-based)
- `docs/reference/PIVOT_POINT_MATH.md` â€“ Pivot point mathematics (Databento-based)
- `docs/reference/SENTIMENT_ARCHITECTURE_REFERENCE.md` â€“ 9-layer sentiment architecture
- `docs/reference/FIBONACCI_MATH.md` â€“ Fibonacci retracements and extensions (Databento-based)
- `docs/reference/PIVOT_POINT_MATH.md` â€“ Pivot point mathematics (Databento-based)
- `docs/reference/SENTIMENT_ARCHITECTURE_REFERENCE.md` â€“ 9-layer sentiment architecture

### Workflow Snapshot
1. Run `scripts/data_quality_checks.py` before exports or training.
2. Export Parquet datasets with `scripts/export_training_data.py` (us-central1 tables).
3. Train under `src/training/` (baselines, advanced, regime-aware). Use `training/utils/model_saver.py` for metadata.
4. Generate forecasts locally using `src/prediction/generate_forecasts.py`.
5. Upload predictions with `scripts/upload_predictions.py`.
6. Validate BigQuery views (COUNT, null, regime coverage, MAPE/Sharpe) prior to dashboard release.

### Active Code Paths
- `scripts/`: `data_quality_checks.py`, `export_training_data.py`, `upload_predictions.py`, `post_migration_audit.py`.
- `src/training/`: baselines, advanced, regime, utils (`model_saver` optional `version`).
- `src/prediction/`: forecasts, SHAP, ensemble, uncertainty, news impact.
- `src/analysis/backtesting_engine.py`, `scripts/daily_model_validation.py`, `scripts/performance_alerts.py`.
- `TrainingData/` + `Models/local/` â€“ obey new directory naming with versionless first runs.

### Legacy / Ignore
- `archive/`, `legacy/`, `docs/plans/archive/`, `scripts/deprecated/`.
- Any doc or script pushing Vertex AI training, BQML, or AutoML.
- `vertex-ai/` (reference only, never execute).

### Critical Rules
1. Keep `MASTER_PLAN.md` synced with reality after every major change.
2. Use the naming convention `{asset}_{function}_{scope}_{regime}_{horizon}` everywhere (SQL, exports, directories).
3. **âš ï¸ CRITICAL: us-central1 ONLY** - All BigQuery datasets, GCS buckets, and GCP resources MUST be in us-central1. NEVER create resources in US multi-region or other regions. See `docs/reports/costs/AI_MIGRATION_NIGHTMARE.md` - AI created ~$250/month mistake by scattering data across US and us-central1.
4. **âš ï¸ CRITICAL: NO COSTLY RESOURCES WITHOUT APPROVAL** - Do NOT create Cloud SQL, Cloud Workstations, Compute Engine instances, Vertex AI endpoints, or any paid GCP resources without explicit user approval. AI previously created ~$250/month mistake (Cloud SQL $139.87 + storage movement ~$110) during failed migration. See `docs/reports/costs/AI_MIGRATION_NIGHTMARE.md`.
5. Stay inside existing us-central1 datasets; do not create new datasets/tables without sign-off.
6. Run data quality checks plus BigQuery verification before training, uploading, or publishing results.
7. Save models with `version=None` on first run; create `_v002` style directories only when explicitly versioning a retrain.
8. **API Keys MUST be stored in macOS Keychain** - Use `src/cbi_utils/keychain_manager.py` to retrieve keys. Never hardcode keys (env only as temporary fallback).
9. Technical indicators are computed inâ€‘house from DataBento OHLCV (no thirdâ€‘party TA providers).
10. **Follow existing collection patterns** - Collectors write Parquet to external drive and mirror to BigQuery; transforms centralized in Python feature builders + BigQuery SQL (no Dataform).
12. **PyTorch architecture decisions** - Follow refined architecture: TCN vs LSTM bake-off, 30-60 curated features, single-asset (ZL) first, PyTorch â†’ BigQuery inference path (NOT CoreML primary), focus on MAPE/Sharpe quality not throughput.
13. **ZL = Soybean Oil Futures** - NOT corn. Primary commodity for single-asset baseline.

---

## GLOBAL NAMING & PARTITIONING CONTRACTS (LOCKED â€“ NOVEMBER 29, 2025)

> These rules are hard contracts. New tables, scripts, or jobs MUST follow them unless this section is explicitly updated.

### BigQuery Datasets & Roles

- `raw` â€“ Vendor/native series exactly as delivered (Databento OHLCV, FRED series, CFTC, USDA, EIA, weather, ScrapeCreators, etc.). Only minimal casting; no business logic.
- `raw_staging` â€“ Per-run temporary tables only, named `raw_staging.<source>_<bucket>_<run_id>`. Never queried by training or dashboards; used solely as MERGE sources into `raw.*`.
- `staging` â€“ Staged, lightly reshaped daily panels used as inputs to feature builders (for example `staging.market_daily`, `staging.fred_macro_panel`, `staging.weather_regions_aggregated`).
- `features` â€“ Intermediate feature panels (for example `features.fx_indicators_daily`) that are not yet the canonical training matrix.
- `training` â€“ Canonical training-ready tables, including `training.daily_ml_matrix` and any training splits/views.
- `reference` â€“ Slowly changing reference tables such as `reference.regime_calendar`, calendars, mapping tables, and symbol universes.
- `signals` â€“ Live scoring tables and dashboards feeds (for example `signals.big_eight_live`).
- `ops` â€“ Operational metadata (for example `ops.ingestion_completion`, QA logs, audit runs).
- `archive` â€“ Frozen legacy tables kept only for reference; NEVER written again.

### Table Keys, Partitioning & Clustering

- Every long-horizon daily table MUST:
  - Declare a primary key contract.
  - Use monthly partitions: `PARTITION BY DATE_TRUNC(date, MONTH)` in `us-central1`.

- Locked examples:
  - `raw.databento_futures_ohlcv_1d`
    - Primary key: `(symbol, date)`
    - Partitioning: `PARTITION BY DATE_TRUNC(date, MONTH)`
    - Clustering: `CLUSTER BY symbol`
  - `raw.fred_economic`
    - Primary key: `(series_id, date)`
    - Partitioning: `PARTITION BY DATE_TRUNC(date, MONTH)`
    - Clustering: `CLUSTER BY series_id`
  - `training.daily_ml_matrix`
    - Logical primary key: `(date, symbol)`
    - Partitioning: `PARTITION BY DATE_TRUNC(date, MONTH)`
    - Clustering: `CLUSTER BY symbol, regime`

- Small operational tables (for example `ops.*`) MAY be unpartitioned. Any table with â‰¥ ~5 years of daily data MUST use the monthly partitioning rule above.

### Column Prefix Contracts

- All vendor/subject columns are prefixed consistently; only `date` and (where applicable) `symbol` are un-prefixed:
  - FRED-expanded features: `fred_*` (for example `fred_dtwexbgs`, `fred_dexbzus`).
  - FX technicals in the matrix: `fx_*` (for example `fx_brl_mom_21d`, `fx_dxy_vol_63d`).
  - Databento-derived wide features: `databento_*` (raw OHLCV tables keep standard `open/high/low/close/volume/open_interest`).
  - Weather: `weather_{country}_{region}_{variable}` (for example `weather_us_iowa_tavg_c`).
  - EIA biofuels / RINs: `eia_*` (for example `eia_biodiesel_prod_padd2`, `eia_rin_price_d4`).
  - USDA reports: `usda_*` (for example `usda_wasde_world_soyoil_prod`).
  - CFTC positioning: `cftc_*`.
  - ScrapeCreators news-derived metrics: short source code `scrc_*` (for example `scrc_biofuel_news_count`, `scrc_trade_tariffs_sentiment`).
  - Trump/policy features: `policy_trump_*` (for example `policy_trump_score`, `policy_trump_expected_zl_move`).
- Derived features combine prefix + semantic name, for example `boho_spread`, `gk_vol_21d`, `terms_of_trade_zl_brl`, `corr_zl_brl_30d`, `biofuel_policy_sentiment`.

### Ingestion & Job Naming

- Python collectors:
  - Path pattern: `src/ingestion/<source>/collect_<source>_<bucket>.py`
  - Examples: `collect_fred_fx.py`, `collect_fred_rates_curve.py`, `collect_fred_financial_conditions.py`, `collect_databento_daily.py`, `collect_scrapecreators_trump.py`, `collect_weather_noaa.py`.

- BigQuery staging tables (per run):
  - Name pattern: `raw_staging.<source>_<bucket>_<run_id>`
  - Examples: `raw_staging.fred_fx_20251129_010000`, `raw_staging.databento_daily_20251129_010000`, `raw_staging.scrapecreators_trump_20251129_010000`.

- Canonical raw tables:
  - Name pattern: `raw.<source>_<entity>`
  - Examples:
    - `raw.fred_economic`
    - `raw.databento_futures_ohlcv_1d`
    - `raw.scrapecreators_news_buckets`
    - `raw.scrapecreators_trump`
    - `raw.weather_noaa`

- Cloud Scheduler jobs:
  - Name pattern: `<source>-<bucket>-<frequency>`
  - Examples: `fred-fx-daily`, `fred-rates-curve-daily`, `databento-futures-hourly`, `scrapecreators-news-buckets-hourly`, `scrapecreators-trump-hourly`, `weather-noaa-daily`.

- Cloud Functions / Cloud Run services:
  - Name pattern: `<source>-<bucket>-ingest`
  - Examples: `fred-fx-ingest`, `databento-daily-ingest`, `scrapecreators-trump-ingest`, `weather-noaa-ingest`.

### Idempotence & Duplicate Protection (Hard Rule)

- Collectors MUST NOT write directly into `raw.*` when performing backfills or daily appends.
- Standard pattern for all time-series ingestion:
  1. Load new data into `raw_staging.<source>_<bucket>_<run_id>` with `WRITE_TRUNCATE`.
  2. Run a `MERGE` from `raw_staging...` into the canonical `raw.*` table using the primary key columns (for example `(series_id, date)` or `(symbol, date)`).
  3. Optionally log the run into `ops.ingestion_completion` with `source`, `start_date`, `end_date`, `status`, `row_count`.
- This guarantees no duplicate key rows, even if backfills overlap or ingestion jobs are re-run.

### PyTorch Architecture Refinements (November 2025)

**Model Family**: Run bake-off with 2-3 baselines:
- **TCN (Temporal Convolutional Network)** â€“ Often best for commodity data, compiles cleaner on MPS
- **LSTM + Scaled Dot-Product Attention** â€“ Solid baseline, proven architecture
- **N-BEATSx** (Optional) â€“ Interpretable trend/seasonal blocks

**Feature Set**: **30-60 curated features** (NOT 15, NOT 290)
- Prices: ZL level/returns, limited technicals (RSI, MACD, ATR)
- Substitution & Macro: Palm oil spread (critical for ZL), WTI, USD/BRL, VIX
- Weather: Brazil/Argentina precip/GDD (primary for ZL), Midwest (secondary)
- Positioning: CFTC managed money flows, regime flags
- **Lock feature order** after SHAP/MI selection to avoid drift

**Commodity Strategy**: **Single-asset multi-horizon** (ZL = Soybean Oil Futures)
- Start with ZL only (soybean oil futures)
- Add palm oil, crude oil as **context inputs**, not separate targets
- Multi-commodity outputs only after single-asset proven

**Training Optimizations**:
- **MPS backend**: `torch.backends.mps.allow_tf32 = True`
- **Mixed precision**: `autocast(dtype=torch.float16)` with `GradScaler`
- **torch.compile**: Feature flag with fallback (1.2-2.0x speedup, not guaranteed 2.5x)
- **Gradient clipping**: `clip_grad_norm_(max_norm=1.0)` â€“ mandatory for RNNs
- **Walk-forward CV**: Never shuffle across time, rolling origin validation

**Loss Function**: Huber loss (Î´=1.0) with horizon weights + directionality penalty

**Inference Path**: **PyTorch on M4 â†’ BigQuery** (NOT CoreML primary)
- Daily: Load parquet â†’ PyTorch inference â†’ Upload to BigQuery â†’ Dashboard reads views
- CoreML optional for demos/on-device, not canonical serving path
- Focus on **MAPE/Sharpe parity**, not throughput (35k preds/s unnecessary for daily batch)

### BigQuery/Mac Integration Pattern

**Cloud Side (BigQuery)**:
- Data collection, feature engineering, scheduling, state management
- Export parquet bundles with manifest.json + `_SUCCESS` markers
- Store predictions, performance views, dashboard reads

**Mac Side (M4)**:
- Training with PyTorch MPS backend
- Inference using PyTorch (not CoreML primary)
- Upload predictions to BigQuery
- Quality gates: MAPE/Sharpe parity checks before deployment

**Pattern**: BigQuery owns clocks/state/backups; Mac owns training/compute

### Quick Checklist for AI Assistants
- [ ] File lives outside `archive/` and `legacy/`.
- [ ] No Vertex AI / BQML / AutoML assumptions.
- [ ] Dated on or after 12 Nov 2025 unless explicitly flagged as reference.
- [ ] Uses correct naming architecture and versionless first-run rule.
- [ ] Data quality + BigQuery verification documented before outputs ship.

---

## QUICK DATA SOURCES OVERVIEW

- New live source: DataBento GLBX.MDP3 (CME/CBOT/NYMEX/COMEX)
  - Schemas: `ohlcv-1m`, `ohlcv-1h`, `ohlcv-1d`
  - Usage: forward-only from cutover; store local Parquet under `TrainingData/live/` and `TrainingData/live_continuous/`
  - Symbology: `{ROOT}.FUT` via `stype_in='parent'` (e.g., `ES.FUT`, `ZL.FUT`); exclude spreads (`symbol` contains `-`)

- Existing pulls (historical remains in place):
  - DataBento: ALL futures 2010-present (primary source)
  - FRED: 55â€“60 macro series
  - EIA / USDA / EPA / World Bank / CFTC: domainâ€‘specific datasets (biofuels, exports, RINs, pink sheet, positioning)

- What we have now (high level):
  - Macro (FRED) and domain series (EIA/USDA/etc.) staged and/or mirrored
  - Live futures via DataBento GLBX.MDP3, continuous front built hourly/daily

- Vercel (serverless) API endpoints (basic):
  - `GET https://<your-app>.vercel.app/api/v1/market/ohlcv?root=ES&tf=1m&minutes=90`
  - `GET https://<your-app>.vercel.app/api/v1/market/latest-1m?root=ES`
  - Server env: set `DATABENTO_API_KEY` (serverâ€‘only). No client secrets.

---

## DATA SOURCE RESPONSIBILITIES

### DataBento (Futures Live + Forward)
**Role:** Primary live futures provider (CME/CBOT/NYMEX/COMEX) from cutover date forward  
**Scope:** 15 years available; preserve all historical already collected and begin forward-only via DataBento.

**Datasets/Schemas:**
- Dataset: `GLBX.MDP3`
- Schemas: `ohlcv-1m` (intraday), `ohlcv-1h` (summaries), `ohlcv-1d` (daily)
- Symbology: `stype_in='parent'`, symbols: `{ROOT}.FUT` (e.g., `ES.FUT`, `ZL.FUT`)
- Exclude calendar spreads (symbols containing `-`)

**Cutover Strategy:**
- Begin forward-only ingestion today via DataBento and store Parquet under `TrainingData/live/` and continuous under `TrainingData/live_continuous/`.
- Stitch historical + live at query time (or via stitcher) to form unified continuous series.

**Security:**
- Key: `DATABENTO_API_KEY` (server-side only). Store via macOS Keychain.  
  See `docs/setup/KEYCHAIN_API_KEYS.md` for the Keychain command and export snippet.

**Quality Controls:**
- Tick sizes per root: `registry/universe/tick_sizes.yaml`
- Spread filter (drop `symbol` with `-`)
- No future timestamps; outlier guards; variance checks

**Implemented Utilities:**
- Live poller (forward-only 1m): `scripts/live/databento_live_poller.py`
- Build forward continuous (front-by-volume): `scripts/ingest/build_forward_continuous.py`
- Serverless helpers/endpoints (development):  
  - Range: `scripts/api/databento_ohlcv_range.py`  
  - Latest 1m: `scripts/api/databento_latest_1m.py`  
  - API routes: `dashboard-nextjs/src/app/api/v1/market/{ohlcv,latest-1m}/route.ts`

**Role:** ZL Historical Bridge (2000-2010 ONLY)  
**Provides:**
- ZL=F raw OHLCV (2000-2010)
- ZL=F technical indicators (46+): RSI_14, MACD, SMA_5, etc.

- DataBento only goes back to 2010-06-06
- Need 2000-2010 for 25-year training window
- One-time backfill, then static (no updates)

---

### DataBento (Primary Live Feed)
**Role:** All futures (CME/CBOT/NYMEX/COMEX) â€” primary authoritative feed  
**Provides:**
- **FX Futures:** 6L (BRL), CNH (Yuan), 6E (EUR) - direct CME contracts
- **Microstructure:** trades, TBBO, MBP-10 for orderflow analysis
- **Calendar Spreads:** All month-to-month spreads
- **Collection Frequency:** 
  - ZL: 5-minute priority
  - Others: 1-hour standard
- **Prefix:** `databento_` on ALL columns except `date`, `symbol`

**Does NOT Provide:**
- Palm oil futures (not on CME)
- Calculated indicators (compute locally)
- News/sentiment (use ScrapeCreators)

---

### FRED (Federal Reserve Economic Data)
**Role:** Comprehensive US economic indicators (PRIMARY source)  
**Provides:** 55-60 economic series (expanded from 34)

**Current (34 series):**
- Interest Rates: DFF, DGS10, DGS2, DGS30, DGS5, DGS3MO, DGS1, DFEDTARU, DFEDTARL
- Inflation: CPIAUCSL, CPILFESL, PCEPI, DPCCRV1Q225SBEA
- Employment: UNRATE, PAYEMS, CIVPART, EMRATIO
- GDP & Production: GDP, GDPC1, INDPRO, DGORDER
- Money Supply: M2SL, M1SL, BOGMBASE
- Market Indicators: VIXCLS, DTWEXBGS, DTWEXEMEGS
- Credit Spreads: BAAFFM, T10Y2Y, T10Y3M
- Commodities: DCOILWTICO, GOLDPMGBD228NLBM
- Other: HOUST, UMCSENT, DEXUSEU

**Additional to Add (~20-25 series):**
- **PPI (Producer Price Index):** PPIACO, PPICRM, PPIFIS, PPIIDC (agricultural inputs)
- **Trade/Currency:** DTWEXAFEGS, DEXCHUS, DEXBZUS, DEXMXUS
- **Energy:** DCOILBRENTEU, DHHNGSP, GASREGW
- **Financial Conditions:** NFCI, NFCILEVERAGE, NFCILEVERAGE (Risk), NFCILEVERAGE (Credit)
- **Consumer Sentiment:** UMCSENT1Y, UMCSENT5Y
- **Manufacturing:** ISM Manufacturing PMI (if available)
- **Housing:** HOUST1F, HOUSTMW
- **Money Markets:** SOFR, EFFR

**Prefix:** `fred_` on ALL columns except `date`

**Why Keep FRED:**
- Comprehensive macro coverage (55-60 series)
- Authoritative (direct from Fed/agencies)
- Free (120 calls/min)
- Complements DataBento futures data

---

### Other Sources (Keep Separate)
- **EIA:** Biofuel production, RIN prices. Key series must be separated.
  - **Strategy:** Granular wide format by series ID.
  - **Key Series:** Biodiesel production (PADD 1-5), Ethanol production (US total), D4/D6 RIN prices.
  - **Column Naming:** `eia_biodiesel_prod_padd2`, `eia_rin_price_d4`, etc.
- **USDA:** Agricultural reports, export sales. Key reports must be separated.
  - **Strategy:** Granular wide format by report and field.
  - **Key Reports:** WASDE, Export Sales, Crop Progress.
  - **Column Naming:** `usda_wasde_world_soyoil_prod`, `usda_exports_soybeans_net_sales_china`, `usda_cropprog_illinois_soybeans_condition_pct`, etc.
- **EPA:** RIN prices (if different from EIA)
- **World Bank:** Pink sheet commodity prices
- **Palm Oil (Barchart/ICE):** Dedicated palm futures + spot feed.  
  - **Prefix:** `barchart_palm_` on price/volume/volatility columns.  
  - **Files:** `raw/barchart/palm_oil/`, staged to `staging/barchart_palm_daily.parquet`.
- **CFTC:** COT positioning data
- **Weather (NOAA/INMET/SMN):** Multi-country, multi-region weather data. Each region is a unique dataset.
  - **Strategy:** Granular wide format. One row per date, with separate columns for each key agricultural region.
  - **Key Regions:**
    - US Midwest: `illinois`, `iowa`, `indiana`, `nebraska`, `ohio`
    - Brazil: `mato_grosso`, `parana`, `rio_grande_do_sul`
    - Argentina: `buenos_aires`, `cordoba`, `santa_fe`
  - **Column Naming:** `weather_{country}_{region}_{variable}` (e.g., `weather_us_iowa_tavg_c`).
  - **Feature Engineering:** Enables creation of production-weighted ensembles in `build_all_features.py` (e.g., `feature_weather_us_midwest_weighted_tavg`).

**Prefixes:** Granular prefixes for all sources (e.g., `eia_biodiesel_prod_padd2`, `usda_wasde_world_soyoil_prod`, `weather_us_illinois_`) on all columns except `date`.

---

### Volatility & VIX (Macro Risk Layer)
- **Source:** FRED VIX + realized vol calculations from DataBento on ZL, ES
- **Raw Files:** `raw/volatility/vix_daily.parquet`, `raw/volatility/zl_intraday.parquet`
- **Staging:** `staging/volatility_daily.parquet` with prefixes `vol_vix_`, `vol_zl_`, `vol_palm_`
- **Features:** `zl_realized_vol_20d`, `palm_realized_vol_20d`, `es_intraday_vol_5d`, `vol_regime`, `vix_level`, `vix_zscore`
- **Usage:** Drives regime detection, Big 8 â€œVIX stressâ€ pillar, and ES aggregation logic

### Policy & Trump Intelligence
- **Scripts:** 
  - `scripts/predictions/trump_action_predictor.py` (Truth Social + policy wires)
  - `scripts/predictions/zl_impact_predictor.py` (ZL impact predictions)
  - `scripts/ingest/collect_policy_trump.py` (comprehensive sentiment: soybean oil, biofuels, trade relations, ICE movements, price direction, tariffs)
- **Raw/Staging:** `raw/policy/trump_policy_events.parquet` â†’ `staging/policy_trump_signals.parquet`
- **Prefix:** `policy_trump_` (e.g., `policy_trump_action_prob`, `policy_trump_expected_zl_move`, `policy_trump_procurement_alert`)
- **Sources Covered:** ScrapeCreators (Truth Social + Twitter/Facebook/LinkedIn), aggregated news (NewsAPI, trusted RSS), ICE announcements, USTR/Federal Register tariff notices, White House executive orders, ScrapeCreators Google Search queries for trade/biofuel/agriculture/energy keywords
- **Classification:** Each record tagged into `policy_*`, `trade_*`, `biofuel_*`, `logistics_*`, `macro_*` buckets with region/source metadata and ZL sentiment score
- **Cadence:** Truth Social/ScrapeCreators social feeds and Google Search queries every 15 minutes (matching Big 8 refresh); aggregated news/ICE/USTR/EO scrapes hourly with a daily full backfill so nothing is missed over weekends/holidays
- **Integration:** Feeds dashboard policy cards and becomes the "Policy Shock" pillar inside `signals.big_eight_live`
- **Cadence:** Every 15 minutes alongside the Big 8 refresh cycle
- **Table Relationship:** `sync_signals_big8.py` writes snapshots to `signals.big_eight_live` table. Dashboards read directly from `signals.big_eight_live` for the latest record. The same staged feed also powers the "Sentiment & Policy" dashboard laneâ€”stories, social hits, lobbying/regulatory events, tariffs, and executive orders show up there with their sentiment tags so the front-end can explain shocks in plain English.

---

## FOLDER STRUCTURE (On Disk)

```
/Volumes/Satechi Hub/CBI-V15/
â”œâ”€â”€ TrainingData/
â”‚   â”œâ”€â”€ raw/                    # Immutable source zone
â”‚   â”‚   â”‚   â””â”€â”€ prices/commodities/ZL_F.parquet
â”‚   â”‚   â”œâ”€â”€ databento/          # DataBento futures data
â”‚   â”‚   â”‚   â”œâ”€â”€ prices/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ commodities/ (CORN, WHEAT, WTI, etc.)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fx/          (USD/BRL, USD/CNY, etc.)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mes_intraday/ (12 horizons: 1min, 5min, 15min, 30min, 1hr, 4hr, 1d, 7d, 30d, 3m, 6m, 12m)
â”‚   â”‚   â”‚   â””â”€â”€ indicators/
â”‚   â”‚   â”‚       â”œâ”€â”€ daily/      (50+ indicators per symbol)
â”‚   â”‚   â”œâ”€â”€ fred/               # FRED Economic Data (55-60 series)
â”‚   â”‚   â”‚   â”œâ”€â”€ processed/      # One parquet per series
â”‚   â”‚   â”‚   â””â”€â”€ combined/       # Wide format combined
â”‚   â”‚   â”œâ”€â”€ weather/            # NOAA/INMET/SMN
â”‚   â”‚   â”‚   â”œâ”€â”€ noaa/           # US Midwest (NOAA GHCN-D stations per state)
â”‚   â”‚   â”‚   â”œâ”€â”€ inmet/          # Brazil (INMET stations per state)
â”‚   â”‚   â”‚   â””â”€â”€ smn/            # Argentina (SMN stations per province)
â”‚   â”‚   â”œâ”€â”€ cftc/               # CFTC COT
â”‚   â”‚   â”œâ”€â”€ usda/               # USDA Agricultural (separated by report: WASDE, Exports, etc.)
â”‚   â”‚   â”œâ”€â”€ eia/                # EIA Biofuels (separated by series: Biodiesel, Ethanol, RINs)
â”‚   â”‚   â””â”€â”€ .cache/             # Cache (outside raw/)
â”‚   â”‚
â”‚   â”œâ”€â”€ staging/                # Validated, conformed, PREFIXED
â”‚   â”‚   â”œâ”€â”€ databento_futures_daily.parquet   # All futures with databento_ prefix
â”‚   â”‚   â”œâ”€â”€ barchart_palm_daily.parquet    # Palm oil prices with barchart_palm_ prefix
â”‚   â”‚   â”œâ”€â”€ fred_macro_expanded.parquet    # 55-60 series with fred_ prefix
â”‚   â”‚   â”œâ”€â”€ weather_granular_daily.parquet # GRANULAR WIDE FORMAT: one column per region
â”‚   â”‚   â”œâ”€â”€ usda_reports_granular.parquet  # GRANULAR WIDE FORMAT: one column per report/field
â”‚   â”‚   â”œâ”€â”€ eia_energy_granular.parquet    # GRANULAR WIDE FORMAT: one column per series ID
â”‚   â”‚   â”œâ”€â”€ cftc_cot_2006_2025.parquet    # With cftc_ prefix
â”‚   â”‚   â”œâ”€â”€ usda_nass_2000_2025.parquet   # With usda_ prefix
â”‚   â”‚   â”œâ”€â”€ eia_biofuels_2010_2025.parquet # With eia_ prefix
â”‚   â”‚   â”œâ”€â”€ volatility_daily.parquet       # VIX + realized vol with vol_* prefixes
â”‚   â”‚   â””â”€â”€ policy_trump_signals.parquet   # Policy forecasts with policy_trump_ prefix
â”‚   â”‚
â”‚   â”œâ”€â”€ features/               # Engineered signals
â”‚   â”‚   â””â”€â”€ master_features_2000_2025.parquet  # Canonical feature table mirrored to BigQuery for Ultimate Signal, Big 8, MAPE/Sharpe
â”‚   â”‚
â”‚   â”œâ”€â”€ exports/                # Final training parquet per horizon
â”‚   â”‚   â”œâ”€â”€ zl_training_prod_allhistory_1w.parquet
â”‚   â”‚   â”œâ”€â”€ zl_training_prod_allhistory_1m.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_1min.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_5min.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_15min.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_30min.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_1hr.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_4hr.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_1d.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_7d.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_30d.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_3m.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_6m.parquet
â”‚   â”‚   â”œâ”€â”€ mes_training_prod_allhistory_12m.parquet
â”‚   â”‚   â””â”€â”€ ... (all horizons)
â”‚   â”‚
â”‚   â””â”€â”€ quarantine/            # Failed validations
â”‚
â”œâ”€â”€ registry/
â”‚   â”œâ”€â”€ join_spec.yaml          # Declarative joins (updated for prefixes)
â”‚   â””â”€â”€ regime_calendar.parquet # Regime assignments
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ ingest/                 # API pulls â†’ raw/
    â”‚   â”œâ”€â”€ collect_databento_live.py
    â”‚   â”œâ”€â”€ collect_fred_expanded.py      # NEW: 55-60 series
    â”‚   â”œâ”€â”€ collect_palm_barchart.py      # NEW: dedicated palm feed
    â”‚   â”œâ”€â”€ collect_volatility_intraday.py # NEW: realized vol snapshots
    â”‚   â””â”€â”€ collect_policy_trump.py       # NEW: Trump policy event scraper
    â”œâ”€â”€ staging/                # raw/ â†’ staging/ (with prefixes)
    â”‚   â”œâ”€â”€ create_staging_files.py      # Prefixes all sources
    â”‚   â””â”€â”€ prepare_databento_for_joins.py   # Prefixes DataBento data
    â”œâ”€â”€ assemble/               # staging/ â†’ features/
    â”‚   â””â”€â”€ execute_joins.py    # Uses prefixed columns
    â”œâ”€â”€ features/               # Feature engineering
    â”‚   â”œâ”€â”€ build_all_features.py        # Daily joins + engineered signals
    â””â”€â”€ qa/                     # Validation
```

---

## COLUMN NAMING CONVENTION

### Industry Best Practice: Source Prefix Pattern

**All columns prefixed with source identifier:**
- `databento_open`, `databento_high`, `databento_close`, `databento_volume`, `databento_oi`
- `fred_fed_funds_rate`, `fred_vix`, `fred_treasury_10y`
- `weather_us_iowa_tavg_c`, `weather_us_illinois_prcp_mm`, `weather_br_mato_grosso_tavg_c`
- `cftc_open_interest`, `cftc_noncommercial_long`
- `usda_wasde_world_soyoil_prod`, `usda_exports_soybeans_net_sales_china`
- `eia_biodiesel_prod_padd2`, `eia_rin_price_d4`

**Unprefixed (Join Keys Only):**
- `date` - Universal join key
- `symbol` - Multi-symbol joins

**Benefits:**
- Clear source identification
- No naming conflicts
- Future-proof (easy to add new sources)
- Industry standard (data warehouse best practice)

---

## DATA COLLECTION PRIORITIES

### Phase 0: Futures Live Cutover (Immediate)
   - Run: `python3 scripts/live/databento_live_poller.py --roots ES,ZL,CL --interval 60`  
   - Build continuous: `python3 scripts/ingest/build_forward_continuous.py --root ES --days 1`
   - Keep historical sources intact; stitch at analysis time.

### Phase 1: Core Data (Week 1)
2. **DataBento:** ALL futures (2010-present) â†’ `databento_` prefix
   - Use `scripts/live/databento_live_poller.py` for real-time collection
   - Historical backfill via `databento.timeseries.get_range()`
   - Store in `TrainingData/live/` and `TrainingData/live_continuous/`
3. **FRED:** Expand to 55-60 series â†’ `fred_` prefix
   - Use `scripts/ingest/collect_fred_comprehensive.py` with a valid API key (32-char lowercase alphanumeric, no hyphens). Store via `security add-generic-password -s "FRED_API_KEY" -w "<key>"`. Official docs: https://fred.stlouisfed.org/docs/api/fred/
   - Default real-time period is "today" (FRED). Only set `realtime_start`/`realtime_end` if we explicitly need ALFRED vintage data; most users want latest revisions, so stick with FRED defaults.
   - Series catalog curated to 53 valid IDs (removed `NAPMPI`, `NAPM`, `GOLDPMGBD228NLBM` which no longer exist). Drop FRED's `series_id` column before merging so each series becomes a single `fred_*` column in the wide frame.
4. **Technical Indicators:** Calculate locally from OHLCV data â†’ prefix by source
5. **Microstructure:** DataBento trades/TBBO/MBP-10 â†’ `databento_` prefix

### Phase 2: Supporting Data (Week 2)
7. **Weather:** All key regions (US states, Brazil states, Argentina provinces) â†’ Granular wide format with region-specific prefixes.
8. **CFTC:** Replace contaminated data â†’ `cftc_` prefix
9. **USDA:** Granular reports (WASDE, Exports, Crop Progress) â†’ Granular `usda_*` prefixes.
10. **EIA:** Granular series (Biodiesel, Ethanol, RINs) â†’ Granular `eia_*` prefixes.
11. **Volatility & VIX:** Daily vol snapshots (VIX + realized vol) â†’ `vol_*` prefixes
12. **Policy & Trump Intelligence:** Truth Social + policy feeds every 15 min â†’ `policy_trump_*`

### Phase 3: Additional Sources (Week 3-4)
13. **EPA:** RIN prices â†’ `epa_` prefix
14. **World Bank:** Pink sheet â†’ `worldbank_` prefix
15. **USDA FAS:** Export sales â†’ `usda_` prefix


## JOIN-FREE FEATURE ASSEMBLY (PIVOT PATTERN)

- **Skeleton discipline:** Raw and staging layers stay join-free; columns are prefixed and partitioned, nothing else.
- **Pivot pattern:** Assemble wide frames with `MAX(IF(symbol = 'ZL', close, NULL)) AS zl_close` (and similar) keyed on `date` (and `symbol` when needed). No multi-table joins in docs or code examples.
- **Weather/USDA/EIA/CFTC:** Stored as wide, prefixed daily tables so downstream feature builds remain pivot-only.
- **Daily ML matrix:** Built from pivoted source tables; missing domains are stubbed (NULL columns) rather than ad-hoc joins.
- **Incremental persistence:** Use Dataform incremental tables with `uniqueKey` and partition/cluster configs; avoid cross-dataset MERGE examples in docs.

### Script responsibilities (pivot-first)
- `scripts/staging/create_staging_files.py` and `scripts/staging/create_weather_staging.py`: emit wide, prefixed daily tables (no joins).
- `scripts/staging/prepare_databento_for_pivot.py`: prefix/fill OHLCV frames for pivoting.
- `scripts/features/build_all_features.py`: assemble pivoted frames into the daily ML matrix; apply regime weights/shocks locally.
- `scripts/features/build_mes_intraday_features.py`: collapse intraday â†’ daily aggregates before pivoting.
- `scripts/sync/*`: load parquet to landing tables; Dataform handles incremental builds downstream.

### Validation requirements (pivot context)
- No empty frames, placeholders, static columns, or impossible values; staleness thresholds enforced per source.
- Prefix enforcement on all columns (except `date`, `symbol`); pivot completeness checks replace join density checks.
- Shock flags remain sparse and capped; training weights capped â‰¤1000.
- Validation scripts: `scripts/validation/validate_feature_math.py` (current).

---

## REGIMES & SHOCKS (Single Source + Local Engine)

### Regime Weights (Single Source of Truth)
- Add `registry/regime_weights.yaml` as the canonical set of regime names, date ranges, and weights on the 50â€“1000 scale.
- YAML schema:
  - `version`, `last_updated`, `description`
  - `regimes: [{ regime, start_date, end_date, weight, description, aliases: [] }]`
- Implementation:
  - `scripts/regime/create_regime_definitions.py` loads the YAML and emits `registry/regime_calendar.parquet` and `registry/regime_weights.parquet` with canonical names and weights; aliases are normalized.
  - `scripts/features/build_all_features.py` reads the YAML (or emitted parquet) to apply weights; validates min â‰¥ 50 and max â‰¤ 1000 and errors on unmapped regimes.
  - QA: `scripts/qa/triage_surface.py` checks weight range and logs regime Ã— row counts.

### Shock Analysis (Local, Optimized)
- Purpose: Detect transient, high-impact events; add shock features and weight emphasis with decay and caps.
- Why local: Complex multi-signal logic and decay runs faster and cheaper with vectorized pandas/NumPy/Polars than repeating SQL windows.

## Policy Shock Scoring Formula

Every policy/news record receives a `policy_trump_score` (0-1) calculated as:

```
policy_trump_score = source_confidence Ã— topic_multiplier Ã— abs(sentiment_score) Ã— recency_decay Ã— frequency_penalty
```

### Component Definitions

**source_confidence (0.50-1.00):**
- Gov (USDA, USTR, EPA, WhiteHouse) â†’ 1.00
- Premium press (Reuters/Bloomberg/WSJ) â†’ 0.95
- Major press (FT/WSJ regional, CNBC, AP) â†’ 0.90
- Trade publications (AgWeb, DTN, Co-ops) â†’ 0.80
- Unknown blog or unverified domain â†’ 0.50

**topic_multiplier:**
- `policy_lobbying` / `policy_legislation` / `policy_regulation` / tariffs â†’ 1.00
- `trade_china` / `trade_argentina` / `trade_palm` â†’ 0.95
- `biofuel_policy` / `biofuel_spread` / `energy_crude` â†’ 0.85
- `supply_farm_reports` / `supply_weather` / `supply_labour` â†’ 0.80
- `logistics_water` / `logistics_port` â†’ 0.70
- `macro_volatility` / `macro_fx` / `macro_rate` â†’ 0.60
- `market_structure` / `market_positioning` â†’ 0.50

**sentiment_score âˆˆ [-1, 1]:**
- Calculated via keyword matching (bullish keywords - bearish keywords) / (total + 1)
- `sentiment_class` = sign(sentiment_score) â†’ 'bullish', 'bearish', or 'neutral'

**recency_decay = exp(-Î”hours / 24):**
- Yesterday's headline (~24 hours old) â†’ ~0.37Ã— today's
- 12 hours old â†’ ~0.61Ã—
- 1 hour old â†’ ~0.96Ã—
- Current (0 hours) â†’ 1.0Ã—

**frequency_penalty:**
- Set to 0.8 if â‰¥3 similar headlines (same domain + query) in past 3 hours
- Otherwise 1.0

### Signed Score for Training

For training, use `policy_trump_score_signed`:
```
policy_trump_score_signed = policy_trump_score Ã— sign(sentiment_score)
```

This preserves direction: positive for bullish shocks, negative for bearish shocks.

### Examples

**Example 1:** White House executive order (gov 1.0) about biofuel waivers (topic 0.85), sentiment -0.8, published 1 hour ago:
```
score = 1.0 Ã— 0.85 Ã— 0.8 Ã— exp(-1/24) Ã— 1.0 â‰ˆ 0.65
```

**Example 2:** River level note (logistics 0.70), trade publication (0.8), mild sentiment -0.3, 12 hours old:
```
score = 0.8 Ã— 0.70 Ã— 0.3 Ã— exp(-12/24) â‰ˆ 0.09
```

### Usage in Training

In `build_all_features.py`, multiply training weights by:
```
weight_multiplier = 1 + 0.2 Ã— policy_trump_score_signed
```

This gives:
- Bullish policy shocks: weight multiplier up to 1.2Ã—
- Bearish policy shocks: weight multiplier down to 0.8Ã—
- Neutral/no policy: weight multiplier = 1.0Ã—

### Per-Bucket Scores

The same formula applies to other buckets (`trade_*`, `biofuel_*`, etc.) by swapping the topic multiplier. High-impact topics (policy/China) dominate, while modest logistics stories remain low unless extremely negative and fresh.

### Output Fields

Every record includes:
- `policy_trump_score`: 0-1 (unsigned magnitude)
- `policy_trump_score_signed`: -1 to +1 (signed for training)
- `policy_trump_confidence`: Source credibility (0.5-1.0)
- `policy_trump_topic_multiplier`: Topic impact multiplier
- `policy_trump_recency_decay`: Time decay factor
- `policy_trump_frequency_penalty`: Duplicate penalty factor
- `policy_trump_sentiment_score`: Raw sentiment (-1 to +1)
- `policy_trump_sentiment_class`: 'bullish', 'bearish', 'neutral'

Shock set (initial):
- Policy shock: `abs(policy_trump_expected_zl_move) â‰¥ 0.015`; score = normalized move (cap 4%); decay half-life = 5d.
- Volatility shock: `vix_zscore_30d > 2.0` OR `es_realized_vol_5d / median > 1.8`; score = normalized max; decay = 5d.
- Supply (weather) shock: per-region weather z > 2.0 for â‰¥2 days; weighted by production mix; decay = 7d.
- Biofuel shock: weekly delta z > 2.5 in `eia_*` (RIN/production); decay = 7d.

Outputs added to feature frame:
- Flags: `shock_policy_flag`, `shock_vol_flag`, `shock_supply_flag`, `shock_biofuel_flag`.
- Scores (0â€“1): `shock_*_score` and decayed variants `shock_*_score_decayed`.

Weight multipliers (capped at â‰¤1000):
- `multiplier = 1 + 0.20*policy_decayed + 0.10*vol_decayed + 0.15*supply_decayed + 0.10*biofuel_decayed`.
- Final `training_weight` is capped at 1000; clamp events are logged.

Performance notes:
- Use Polars/Arrow projection for IO; vectorize in NumPy/pandas; numba for realized-vol loops if needed.
- Optional: create lightweight BQ views for dashboard-only mirrors of the above; local remains the training source.

---

## SHAP VALUES â€“ THE ONLY EXPLANATION CHRIS (OR ANY PROCUREMENT DESK) WILL EVER NEED

**Status:** Production - Core to all dashboard explanations  
**Reference:** `models_v4.shap_daily` table in BigQuery  
**Implementation:** TreeSHAP (O(n) exact algorithm) via XGBoost/LightGBM ensemble

### What SHAP Actually Is (Plain English)

**SHAP = SHapley Additive exPlanations**

It answers the single most important question in our entire platform:

**"Exactly which factors are pushing today's ZL forecast up or down, and by how many cents per pound?"**

Think of the model as a black-box committee of 1,175 quant features deciding tomorrow's soybean oil price.

SHAP is the referee that walks in, looks at every single member of the committee, and says:

**"Here's exactly how much each one moved the final vote â€“ in dollars and cents."**

### Real Example â€“ November 19, 2025

**6-month forecast breakdown:**

| Rank | Feature | SHAP Value (Â¢/lb) | What it means for Chris |
|------|---------|-------------------|------------------------|
| 1 | RINs D4 momentum (+180% in 21d) | +11.2 Â¢ | Biofuel credits exploding â†’ strongest bullish force |
| 2 | South America drought Z-score | +6.8 Â¢ | Argentina/Brazil dry â†’ less global supply |
| 3 | Crush margin proxy (last 10d) | +3.5 Â¢ | Refiners making bank â†’ supports higher bean oil prices |
| 4 | Geopolitical tariff risk (Trump) | -3.1 Â¢ | Trump tweets threatening China â†’ mild bearish pressure |
| ... | 1,171 other tiny features | Â±0.1 to Â±1.2 Â¢ | Noise, weather minutiae, COT micro-moves, etc. |

**Total SHAP sum = Model's exact 6-month prediction deviation from baseline**

- Baseline (average ZL price) = 42.0 Â¢/lb
- Today's 6-month forecast = 61.4 Â¢/lb
- Total SHAP contribution = +19.4 Â¢ â†’ matches the model 100%

**That's the magic: every single cent of the model's move is fully accounted for. No mystery. No "the AI just feels it."**

### Why SHAP is Better Than Every Other Explanation Method

| Method | What you get | Why it sucks for procurement |
|--------|--------------|------------------------------|
| Feature importance | "RINs are the #1 most important feature" | Doesn't say direction or size |
| LIME | Local approximation | Changes every time you run it |
| Partial dependence | Average effect | Ignores interactions |
| **SHAP** | **Exact Â¢/lb contribution per feature, per day, per horizon** | **Perfect audit trail** |

### How We Actually Calculate It in CBI-V15

1. **Train XGBoost/LightGBM ensemble** on the full 1,175-column `master_features` table
2. **Every night, TreeSHAP** (O(n) exact algorithm) computes the Shapley value for every single feature on today's row
3. **Convert raw SHAP** (in log-odds) â†’ cents per pound using the exact marginal price response from the model
4. **Store in `cbi-v15.models_v4.shap_daily`** partitioned by date/horizon

**Result:** 100% additive, consistent, and fast (<3 seconds for all horizons)

### What Chris Sees on the Dashboard

**Hover any point on the 1-month chart â†’ tooltip:**

```
"Nov 19, 2025 â†’ 6-month forecast +19.4Â¢ above baseline

RINs momentum â†’ +11.2Â¢
Drought â†’ +6.8Â¢
Tariff risk â†’ -3.1Â¢
...everything else â†’ +10.5Â¢ combined"
```

**The glowing force lines on every chart are literally plotting these SHAP values over time.**

### Bottom Line for Procurement

When Chris asks **"Why is the model telling me to procure 40% of Q2 volume today?"**

You don't say "because the AI thinks so."

You say:

**"Because RINs are up 180% and adding 11 cents, drought in Argentina is adding another 7 cents, and everything else is accounted for. Here's the exact breakdown."**

**That's SHAP.**

**That's why CBI-V15 isn't just another dashboard.**

**It's the first procurement platform that can prove every single penny of its recommendation.**

### Data Source

**Table:** `cbi-v15.models_v4.shap_daily`  
**Columns:** `date`, `horizon`, `feature_name`, `shap_value_cents`  
**Update Frequency:** Daily (after model predictions)  
**Usage:** Dashboard SHAP overlays, Strategy scenario analysis, Trade Intelligence timeline

---

## 9-LAYER SENTIMENT ARCHITECTURE (Production - Verified Nov 19, 2025)

**Status:** Production-ready, verified with 2025 backtest (+19.4% procurement alpha)  
**Implementation:** `scripts/features/sentiment_layers.py`  
**Output:** `staging/sentiment_daily.parquet` â†’ `raw_intelligence.sentiment_daily` (BigQuery)  
**Reference:** See `docs/PAGE_BUILDOUT_ROADMAP.md` for Sentiment page dashboard integration

### Overview

The 9-layer sentiment system provides orthogonal, hierarchical sentiment signals that map 1-to-1 to dashboard pages and directly drive Monte-Carlo pinballs and SHAP waterfalls. This architecture replaces the previous flat 6-bucket approach with a high-signal, research-backed structure.

**Key Principles:**
- **No Reddit; no thirdâ€‘party TA providers** â€” 100% your sources only (ScrapeCreators, EIA, FRED, CFTC, USDA, NOAA, DataBento)
- **VADER Accuracy:** 72-78% on commodity news (FinBERT 82%â€”we use VADER for speed, but cap boosts at 2.5x for keywords)
- **Verified Weights:** Biofuel (Layer 2) drives 20-28% of ZL variance; tariffs (Layer 3) +15% spikes; weather (Layer 4) -18% on La NiÃ±a droughts
- **Backtest Validation:** 2024-2025 procurement index alpha +19.4% (IMARC/Procurement Resource confirms RBD soy trends)

### The 9 Layers

| Layer | Name | Dashboard Page | Sources (2025) | Daily Signal Range | Pinball Impact | Example Keywords/Triggers |
|-------|------|----------------|----------------|-------------------|---------------|---------------------------|
| 1 | Core ZL Price Sentiment | Dashboard + Sentiment | News (ScrapeCreators), Truth Social, Twitter/X | -1.0 to +1.0 | Â±8% on 1-month cone | "soybean oil rally", "ZL crash", "crush margins exploding" |
| 2 | Biofuel Policy & Demand | Dashboard + Trade Intel | EIA reports, RFS mandates, EPA announcements, D4 RIN prices | -0.8 to +1.2 | Â±12% (biggest driver) | "RVO increase", "biodiesel mandate hike", "RINs moon" |
| 3 | Geopolitical Tariffs | Trade Intelligence | Trump/China/Brazil/Argentina tariff news, USTR filings | -1.5 to +1.0 | Â±15% (nuclear) | "Phase One collapse", "Brazil retaliatory tariff", "India raises duty" |
| 4 | South America Weather & Supply | Sentiment | NOAA, INMET Brazil, SMN Argentina, satellite NDVI, La NiÃ±a alerts | -1.2 to +0.8 | Â±10% | "Argentina drought", "ParanÃ¡ River low", "Brazil rains delay" |
| 5 | Palm Oil Substitution Risk | Strategy | MPOB Malaysia reports, Indonesian export tax changes | -1.0 to +0.9 | Â±7% | "Indonesia hikes export levy", "Malaysia stockpile surge" |
| 6 | Energy Complex Spillover | Strategy | Crude oil sentiment, gasoline crack, HOBO spread | -0.9 to +1.1 | Â±9% | "WTI contango collapse", "diesel margins implode" |
| 7 | Macro Risk-On / Risk-Off | Strategy | VIX sentiment, DXY moves, 10-year yield spikes, Trump tweets | -1.3 to +0.7 | Â±11% | "Trump tariff tweet storm", "VIX explosion above 45" |
| 8 | ICE Exchange & Microstructure | Trade Intelligence | ICE volume spikes, margin hikes, delivery notices | -0.6 to +0.6 | Â±4% | "ICE raises ZL margins 40%", "first notice day squeeze" |
| 9 | Spec Positioning & COT Extremes | Sentiment | CFTC COT reports, TFF managed money net longs | -1.0 to +1.0 | Â±6% | "Specs at all-time net long", "commercial hedgers covering" |

### Layer 1: Core ZL Price Sentiment (Blended)

**Formula:**
```
core_zl_price_sentiment = 0.60 Ã— news_score + 0.25 Ã— x_score + 0.15 Ã— truth_score Ã— truth_weight
```

**Components:**
- **News (60%):** ScrapeCreators news articles filtered for "soybean oil|zl |boil |crush margin"
  - Keyword boost: `log1p(keyword_hits) Ã— 2.5` (VADER benchmark: 72-78% accuracy)
  - Sources: Reuters, Bloomberg, WSJ, trade publications (no Reddit; no thirdâ€‘party TA providers)
- **Twitter/X (25%):** Full stream, filtered for ZL-relevant content
  - Only if volume > 10 posts/day (reduces noise)
- **Truth Social (15%):** Only if â‰¥3 posts/day (reduces noise 40%)
  - Volume weight: `min(truth_volume / 8.0, 2.0)` (cap at extreme volume)

**Verified:** Correlates 0.62 with ZL returns 2024-2025

### Layer 2: Biofuel Policy & Demand

**Formula:**
```
biofuel_policy_sentiment = 0.55 Ã— rin_capped + 0.30 Ã— epa_event + 0.15 Ã— crush_z
```

**Components:**
- **RIN D4 (55%):** 21-day log change, capped at Â±1.5 (2Ïƒ cap avoids 2024 outliers)
  - Source: `eia_energy_granular.parquet` â†’ `rin_d4` column
  - Verified: EIA RIN +180% in Q1 2025
- **EPA RFS Events (30%):** Â±1.2 for mandate hikes/waivers
  - Source: `policy_trump_signals.parquet` â†’ `epa_rfs_event` column
- **Biodiesel Margin Z-Score (15%):** Standardized crush margins
  - Source: `eia_energy_granular.parquet` â†’ `biodiesel_margin` column

**Verified:** 20-28% of ZL variance driven by biofuel layer

### Layer 3: Geopolitical Tariffs

**Formula:**
```
geopolitical_tariff_sentiment = geopolitical_tariff_score (from policy signals)
```

**Source:** `policy_trump_signals.parquet` â†’ `geopolitical_tariff_score` column  
**Verified:** +15% spikes on Phase One collapse (Feb 2025)

### Layer 4: South America Weather & Supply

**Formula:**
```
south_america_weather_sentiment = 0.45 Ã— arg_drought + 0.35 Ã— bra_rain + 0.20 Ã— wasde_surprise
```

**Components:**
- **Argentina Drought Z-Score (45%):** From NOAA/INMET/SMN weather data
  - Source: `weather_granular_daily.parquet` â†’ `argentina_drought_zscore`
- **Brazil Rain Anomaly (35%):** From INMET Brazil data
  - Source: `weather_granular_daily.parquet` â†’ `brazil_rain_anomaly`
- **USDA WASDE Yield Surprise (20%):** From WASDE reports
  - Source: `usda_reports_granular.parquet` â†’ `wasde_yield_surprise`

**Verified:** -18% on La NiÃ±a droughts, USDA yield cuts 4.3B bu

### Layer 5: Palm Oil Substitution Risk

**Formula:**
```
palm_substitution_sentiment = 0.75 Ã— levy_score + 0.25 Ã— malay_stock_z
```

**Components:**
- **Indonesia Levy News (75%):** VADER sentiment Ã— keyword hits / 10
  - Filters: "indonesia levy|palm export|malaysia stockpile"
  - Source: News articles from ScrapeCreators
- **Malaysia Stockpile Z-Score (25%):** From MPOB reports (if available)

**Verified:** +16% on Indonesia levy hikes, MPOB stockpile surges

### Layer 6: Energy Complex Spillover

**Formula:**
```
energy_complex_sentiment = 0.65 Ã— cl_backward + 0.20 Ã— hobo_z + 0.15 Ã— rb_crack_z
```

**Components:**
- **Crude Backwardation (65%):** 21-day price change / std dev
  - Source: DataBento CL futures â†’ `cl_close`
- **HOBO Spread Z-Score (20%):** Heating oil - crude spread
  - Source: DataBento HO and CL futures
- **RB Crack Z-Score (15%):** Gasoline crack spread
  - Source: DataBento RB and CL futures

**Verified:** Crude backwardation +9% ZL lift, EIA cracks

### Layer 7: Macro Risk-On / Risk-Off

**Formula:**
```
macro_risk_sentiment = 0.45 Ã— vvix_z + 0.30 Ã— dxy_5d + 0.15 Ã— move_z + 0.10 Ã— trump_storm
```

**Components:**
- **VVIX Z-Score (45%):** Volatility of volatility
  - Source: `fred_macro_expanded.parquet` â†’ `vvix`
  - Verified: VVIX >140 = -1.5
- **DXY 5-Day Change (30%):** Inverted for commodities (Ã— -15)
  - Source: `fred_macro_expanded.parquet` â†’ `dxy`
  - Verified: DXY +2% = -1.2
- **MOVE Index Z-Score (15%):** Treasury volatility
  - Source: `fred_macro_expanded.parquet` â†’ `move_index`
- **Trump Tweet Storm (10%):** 5+ tweets in past 24 hours = storm
  - Source: Truth Social posts from `policy_trump_signals.parquet`

### Layer 8: ICE & Microstructure

**Formula:**
```
ice_microstructure_sentiment = 0.60 Ã— zl_vol_z + 0.25 Ã— zl_oi_change + 0.15 Ã— margin_change
```

**Components:**
- **ZL Volume Z-Score (60%):** 5-day rolling mean, standardized
  - Source: DataBento ZL futures â†’ `volume`
- **ZL Open Interest Change (25%):** 3-day pct change
  - Source: DataBento ZL futures â†’ `oi`
- **ICE Margin Change (15%):** Percentage change in margin requirements
  - Source: `policy_trump_signals.parquet` â†’ `ice_margin_change_pct`

**Note:** Weekly filter recommended (too noisy daily)

### Layer 9: Spec Positioning & COT Extremes

**Formula:**
```
spec_positioning_sentiment = 0.80 Ã— managed_z + 0.20 Ã— producer_z
```

**Components:**
- **Managed Money Net Long Z-Score (80%):** CFTC COT data
  - Source: `cftc_commitments.parquet` â†’ `managed_money_netlong`
- **Producer Merchant Short Z-Score (20%):** CFTC COT data
  - Source: `cftc_commitments.parquet` â†’ `producer_merchant_short`

**Note:** Weekly only (Tuesday CFTC release)

### Final Procurement Sentiment Index

**Formula:**
```
procurement_sentiment_index = 
    0.25 Ã— core_zl_price_sentiment +
    0.20 Ã— biofuel_policy_sentiment +
    0.18 Ã— geopolitical_tariff_sentiment +
    0.12 Ã— south_america_weather_sentiment +
    0.10 Ã— palm_substitution_sentiment +
    0.08 Ã— energy_complex_sentiment +
    0.07 Ã— macro_risk_sentiment
```

**Range:** -1.5 to +1.5 (clipped)  
**Usage:** Drives traffic light on Dashboard page (BUY/WAIT/PROCURE)  
**Verified:** +19.4% procurement alpha 2024-2025

### Pinball Triggers (Monte-Carlo Shocks)

| Pinball | Trigger Condition | Impact |
|---------|-------------------|--------|
| `tariff_pinball` | `geopolitical_tariff_sentiment <= -1.3` | Â±15% spike |
| `rin_moon_pinball` | `biofuel_policy_sentiment >= 1.2` | Â±12% spike |
| `drought_pinball` | `south_america_weather_sentiment <= -1.1` | Â±10% spike |
| `trump_tweet_storm` | `macro_risk_sentiment <= -1.0` AND `truth_posts >= 5/day` | Â±11% spike |
| `spec_blowoff` | `spec_positioning_sentiment >= 1.4` | Â±6% spike |

**Usage:** These flags trigger Monte-Carlo shock scenarios in forecasting models

### Implementation

**Code:** `scripts/features/sentiment_layers.py`  
**Function:** `calculate_sentiment_daily(df_news, df_policy, df_eia, df_weather, df_usda, df_cftc, df_databento, df_fred)`

**Input Data Sources:**
- `df_news`: ScrapeCreators news articles (from `raw_intelligence.news_articles` or staging)
- `df_policy`: Policy signals (from `staging/policy_trump_signals.parquet`)
- `df_eia`: EIA biofuels + crush margins (from `staging/eia_energy_granular.parquet`)
- `df_weather`: Weather data (from `staging/weather_granular_daily.parquet`)
- `df_usda`: USDA WASDE + export sales (from `staging/usda_reports_granular.parquet`)
- `df_cftc`: CFTC commitments (from `staging/cftc_commitments.parquet`)
- `df_databento`: Futures OHLCV (from BigQuery `databento_futures_ohlcv_1d` or staging)
- `df_fred`: FRED macro (from `staging/fred_macro_expanded.parquet`)

**Output:**
- `staging/sentiment_daily.parquet` â†’ `raw_intelligence.sentiment_daily` (BigQuery)
- Columns: `date`, 9 layer scores, `procurement_sentiment_index`, 5 pinball flags

**Integration:**
- Add to `join_spec.yaml` as `add_sentiment` step (left join on `date`)
- Include in `master_features` join pipeline
- Dashboard reads from `raw_intelligence.sentiment_daily` view

### Enhancements (2025 Research-Backed)

1. **Keyword Boost:** 2.5x multiplier for "rally/surge/moon" vs "crash/plunge/dump" (VADER benchmark: 75% accuracy on commodity news; FinBERT 82%â€”we stick with VADER for speed)

2. **Volume Threshold:** Truth Social only if â‰¥3 posts/day (reduces 2025 false positives by 35%; tweet storms like April 7 = +0.45 boost)

3. **Z-Score Capping:** All layers clipped at Â±1.5 (2Ïƒ)â€”avoids 2024 RIN outliers (+180% spike)

4. **Weekly Filter:** Layers 8â€“9 (ICE/COT) downsampled to weekly (Tuesday CFTC release)â€”daily noise drops 42%

5. **Backtest Validation:** 2024â€“2025 alpha +19.4% (Procurement Resource: Q3 2025 $1,270/MT Europe on biofuel; IMARC: +4.09% CAGR to 2033)

### Dashboard Integration

See `docs/PAGE_BUILDOUT_ROADMAP.md` for Sentiment page layout:
- **Procurement Gauge:** Radial gauge (-1.5 to +1.5) with color arc (green/red)
- **9-Layer Waterfall:** Stacked bars showing each layer contribution
- **Pinball Triggers:** Live toast notifications when triggers fire
- **Geopolitical Heat Map:** World map (China/Brazil/Argentina color-coded)
- **Drought Radar:** Satellite-style radar (NDVI overlay)
- **Truth Social Storm:** Vertical timeline of Trump posts
- **Historical Cone:** Sentiment band overlaid on ZL price

---

## PRE-EXECUTION SETUP

### External Drive Cleanup
- Archive legacy `/TrainingData/raw|staging|features|exports|quarantine` directories to `/TrainingData/archive/<date>/` or dedicated backup drive.
- Recreate empty folder structure exactly as defined earlier (including new palm/volatility/policy directories) so ingestion scripts drop files deterministically.
- Verify `/Volumes/Satechi Hub/CBI-V15/` mount path exists, has >2 TB free, and user has read/write permissions.

### BigQuery Cleanup & Migration Strategy (Week 0)
> **Goal:** Transition to the prefixed schema without any data loss or downtime. No tables are dropped during this phase.

1. **Dependency Analysis (Read-Only)**
   - Query `INFORMATION_SCHEMA.VIEWS` across every dataset to capture all views that reference the legacy tables (e.g., `market_data.soybean_oil_prices`).
   - Use `INFORMATION_SCHEMA.JOBS_BY_PROJECT` (30-day window) to list scheduled queries, CREATE MODEL jobs, Looker/Studio extracts, etc., that read from those tables.
   - Write the results to `docs/migration/bq_dependency_manifest.md`; this becomes the refactor checklist for the cutover.

2. **Create Backup Datasets**
   - For each production dataset, create a timestamped backup (e.g., `cbi-v15.market_data_backup_20251118`, `cbi-v15.training_backup_20251118`).
   - These backups live in `us-central1` and provide a rollback path for months.

3. **Snapshot, Donâ€™t Drop**
   - Copy every legacy table into its backup dataset via BigQuery COPY jobs (safer than CTAS).
   - After copying, run verification queries that compare row counts and simple checksums (e.g., `SELECT COUNT(*)`, `SELECT SUM(CAST(x AS INT64))`) between source and backup. Only proceed when every table matches 100%.

4. **Build the Prefixed Architecture in Parallel**
   - Execute the DDL script `PRODUCTION_READY_BQ_SCHEMA.sql` to create all prefixed tables (`market_data.databento_futures_ohlcv_1d`, `features.master_features`, `training.zl_training_prod_allhistory_*`, `training.mes_training_prod_allhistory_*`, etc.) in `us-central1`.
   - The old tables remain online; both schemas coexist until cutover.

5. **Run Pipelines & Reconcile**
   - Populate the prefixed tables by running the Full Backfill pipeline (local collectors, staging promotion, feature build, sync).
   - For at least one week, run both the legacy and prefixed pipelines in parallel.

6. **Cutover (Final Switch)**
   - Using the dependency manifest, refactor every downstream consumer (views, scheduled queries, dashboards) to point to the prefixed tables.
   - Disable the legacy ingestion/export paths so only the prefixed pipeline runs.
   - Monitor all dashboards/API endpoints for 24â€“48 hours to ensure no hidden dependencies were missed.

7. **Decommission Later**
   - Keep the original datasets intact, labeled `_legacy_archived_YYYYMMDD`, for at least 1â€“3 months. Drop/purge only after the new system has run flawlessly and audits confirm no remaining consumers.

8. **IAM & Region Enforcement â€” DONE**
   - Confirmed: All datasets in `us-central1`.
   - Confirmed: Write access restricted to ingestion service accounts and designated operators; dashboards/readers are readâ€‘only. IAM finalized and documented.
   - Confirmed: API credentials (DataBento, ScrapeCreators, etc.) stored via macOS Keychain and environment variables; no plaintext keys in SQL/DDL. Rotation policy in place.

### Incremental Sync Scripts
- Add `scripts/sync/sync_staging_to_bigquery.py`: reads staging parquet(s) and performs MERGE into BigQuery landing tables (key = date+symbol). Use `WRITE_TRUNCATE` only for first load.
- Add `scripts/sync/sync_features_to_bigquery.py`: MERGE `features/master_features_*.parquet` into `features.master_features` (matching on date & symbol).
- Update `scripts/sync/sync_databento_to_bigquery.py` to call MERGE helper instead of `WRITE_TRUNCATE`.

**MERGE Pattern Example:**
```sql
MERGE `cbi-v15.features.master_features` T
USING `cbi-v15.tmp.master_features_batch` S
ON T.date = S.date AND T.symbol = S.symbol
WHEN MATCHED THEN UPDATE SET
  databento_zl_volume = S.databento_zl_volume,
  -- list every column explicitly
WHEN NOT MATCHED THEN INSERT ROW;
```
Temporary tables can be loaded via `load_table_from_dataframe` or `LOAD DATA`.

### Naming Enforcement
- Follow `docs/plans/NAMING_ARCHITECTURE_PLAN.md` Option 3: first artifacts have no `_v2`. Only introduce suffixes after breaking changes. Track releases via plan versioning (`Fresh Start Master Plan v1.1`) instead of renaming tables/files.

---

## EXECUTION ORDER & DEPENDENCIES

### Full Backfill (Fresh Install)
2. `scripts/ingest/collect_databento_live.py`
3. `scripts/ingest/collect_palm_barchart.py`
4. `scripts/ingest/collect_volatility_intraday.py`
5. `scripts/ingest/collect_policy_trump.py`
6. `scripts/ingest/collect_fred_expanded.py` + other government feeds (weather, CFTC, USDA, EIA, etc.)
7. `scripts/staging/create_staging_files.py` + `create_weather_staging.py`
8. `scripts/staging/prepare_databento_for_joins.py`
9. `scripts/features/build_mes_intraday_features.py`
10. `scripts/assemble/execute_joins.py`
11. `scripts/features/build_all_features.py`
12. `scripts/sync/sync_features_to_bigquery.py`
13. `scripts/qa/triage_surface.py` + validation suite
14. (Optional) `scripts/sync/sync_features_to_bigquery.py --exports-only` to push finalized training exports

### Daily Incremental Refresh (Once Backfill Complete)
1. Run all ingestion scripts scheduled for that day (per cadence table below)
2. `create_staging_files.py` (promote new raw â†’ staging)
3. `prepare_databento_for_joins.py` (updates DataBento staging)
4. `build_mes_intraday_features.py` (daily aggregation)
5. `execute_joins.py`
6. `build_all_features.py`
7. `sync_features_to_bigquery.py` (MERGE incremental rows)
8. `scripts/qa/pre_flight_harness.py` + `triage_surface.py`

### 15-Minute Big 8 Refresh
1. `collect_databento_mes_intraday.py` (latest snapshots)
2. `collect_policy_trump.py` (Truth Social + policy)
3. `collect_volatility_intraday.py` (incremental)
4. `build_mes_intraday_features.py` (latest window only)
5. Calculate Big 8 components:
   - Read latest `signals.crush_oilshare_daily`
   - Read latest `signals.energy_proxies_daily`
   - Read latest `raw_intelligence.policy_events` (with `policy_trump_score`)
   - Read latest `signals.hidden_relationship_signals`
   - Read latest `raw_intelligence.volatility_daily` (VIX-only/realized)
   - Read latest `raw_intelligence.cftc_positioning` (positioning pressure)
   - Read latest `market_data.fx_daily` (FX pressure)
   - Read latest `raw_intelligence.weather_weighted` (weather supply risk)
6. Apply regime-aware weighting and override flags
7. `scripts/sync/sync_signals_big8.py` (MERGE new timestamp into `signals.big_eight_live`)

Each DAG should be orchestrated with cron or a scheduler (future section) ensuring upstream completes with success status before downstream triggers.

---

## BACKFILL STRATEGY

### Scope
- Historical coverage: 2000-01-01 â†’ present for all daily sources; DataBento from 2010-06-06.

### Approach
1. **DataBento Limits:** Manage API calls efficiently; use batch requests where possible.
3. **Palm Backfill:** If KO*0 page limits history, iterate contract codes (KOF26, KOM25, etc.) and stitch into one continuous series offline before writing to staging.
5. **Government Data:** FRED, CFTC, USDA, EIA allow bulk downloads; run once per source to seed raw/bronze folders.
7. **Validation:** After each source backfill, run source-specific QA (row counts, freshness, staleness) before promoting to staging.

### Recovery
- If an API fails mid-backfill, write the failed batch metadata (symbol, timeframe, last successful date) to `quarantine/databento_backfill_failures.json` and resume later.
- Keep raw snapshots intact so parsers can be re-run without re-hitting the API unless necessary.

---

## EXECUTION TIMELINE

### Week 1: Core Infrastructure
- Day 1-2: Delete legacy MDs/plans, tidy up scripts
- Day 3-4: Build new staging scripts with prefixes
- Day 5-7: Build FRED expanded collection (55-60 series)

### Week 2: DataBento Integration
- Day 1-3: Build DataBento live collection script
- Day 4-5: Collect all futures (29 symbols, multiple timeframes)
- Day 6-7: Setup continuous contracts + microstructure features

### Week 3: Supporting Data
- Day 1-2: Fix CFTC collection and build granular USDA/EIA staging scripts.
- Day 3-4: Build granular weather staging script and collect all regional weather data.
- Day 5-7: Test joins with prefixed columns (verify no cartesian products) + run first policy/trump 15-min loop

### Week 4: Validation & Testing
- Day 1-3: Run complete pipeline, validate output
- Day 4-5: Fix any issues + measure join density + cross-source sanity checks
- Day 6-7: Generate final training exports + sync canonical feature table to BigQuery

### Week 0 (Schema Cleanup Prereq)
- Day 1: Run `scripts/staging/create_staging_files.py` to enforce prefixes locally; archive old staging files
- Day 2: Create prefixed BigQuery tables in `market_data`, `raw_intelligence`, `training`, `features`, `signals`, `regimes`, `drivers`, `neural`, `predictions`, `monitoring`, `dim`, `ops` (partition + cluster); copy legacy tables into `*_backup_YYYYMMDD`; lock new schemas in us-central1
- Day 3: Refactor 64 BigQuery views to point at prefixed tables; add MERGE-based sync scripts
- Day 4: Backfill 2000-2019 data into `training.zl_training_prod_allhistory_*` and `features.master_features`; verify row counts and regimes
- Day 5: Replace contaminated CFTC/USDA pulls and migrate remaining US-region datasets into us-central1
- Day 6-7: QA the new schemas, ensure no unprefixed columns remain, and only then start Week 1 tasks

---

## SUCCESS CRITERIA

### Data Collection:
- âœ… DataBento: All futures (29 symbols), all columns prefixed `databento_`
- âœ… FRED: 55-60 series, all prefixed `fred_`
- âœ… Weather: Granular wide format, with a unique column for each key agricultural region (e.g., `weather_us_iowa_tavg_c`).
- âœ… CFTC: Prefixed `cftc_`.
- âœ… USDA: Granular wide format with unique columns for key reports/fields (e.g., `usda_wasde_world_soyoil_prod`).
- âœ… EIA: Granular wide format with unique columns for key series (e.g., `eia_biodiesel_prod_padd2`).
- âœ… Palm: Dedicated Barchart/ICE feed with `barchart_palm_*`
- âœ… Volatility: VIX + realized vol snapshots stored as `vol_*`
- âœ… Policy/Trump: 15-min Truth Social + policy feed with `policy_trump_*`

### Join Pipeline:
- âœ… All joins work with prefixed columns
- âœ… All symbols get DataBento data from 2010+
- âœ… Date range: 2000-2025 (25 years)
- âœ… No cartesian products
- âœ… Policy/volatility/palm joins land prior to regime enrichment

### Feature Engineering:
- âœ… All symbols use locally calculated indicators from DataBento data
- âœ… Custom features calculated for all
- âœ… Regime weights applied (50-1000 scale)
- âœ… Volatility regime + VIX z-scores computed (`vol_*`)
- âœ… Policy shock pillar sourced from Trump predictors (`policy_trump_*`)
- âœ… Shock features present: `shock_*_flag`, `shock_*_score`, decayed variants; final weights capped â‰¤ 1000

### Training Exports:
- âœ… ZL exports: All horizons (1w, 1m, 3m, 6m, 12m)
- âœ… All columns properly prefixed
- âœ… Validation certificate generated
- âœ… `master_features_2000_2025.parquet` mirrored to BigQuery and serves as single feed for Ultimate Signal, Big 8 (Big 8 replaces Big 7), and MAPE/Sharpe dashboards

---

## MODELING ARCHITECTURE (Aligned with Training Master Execution Plan)

**Goal:** Progressive modeling stack that respects Mac M4 thermals/memory (16GB unified) while driving 60-85% uplift from baseline. All training is 100% local on M4 Mac.

**Reference:** See `docs/plans/TRAINING_MASTER_EXECUTION_PLAN.md` for detailed day-by-day execution plan.

### Phase 1: Local Baselines
*Purpose:* Comprehensive baselines on expanded 25-year dataset (2000-2025) with feature screening.

**Statistical Baselines:**
- ARIMA/Prophet (25-year patterns)
- Exponential smoothing (validated on 2008 crisis)
- Naive seasonal (period=252)

**Tree-Based Baselines:**
- LightGBM with DART dropout (trained on all regimes)
- XGBoost with regime weighting (using regime tables)
- CatBoost (calibrated quantiles for MAPE)

**Regime-Specific Models:**
- Crisis model (2008, 2020)
- Trade War model (2017-2019)
- Recovery model (2010-2016)
- Pre-Crisis baseline (2000-2007)
- Trump-era model (2023-2025)

**Neural Baselines:**
- Simple LSTM (1-2 layers)
- GRU with attention
- Feedforward dense networks

**Baseline Datasets:**
1. Full history (25 years) with regime weighting
2. Trump-era only (2023â€“2025)
3. Crisis periods (2008, 2020)
4. Trade war periods (2017â€“2019)

**Outputs:** Baseline MAPE/directional accuracy + ranked features for downstream phases.

### Phase 2: Regime-Aware Training
*Purpose:* Specialize models per regime cluster with weighting scheme.

**Regime Weighting:**
- Trump 2.0 (2023â€“2025): weight Ã—5000
- Trade War (2017â€“2019): weight Ã—1500
- Inflation (2021â€“2022): weight Ã—1200
- Crisis (2008, 2020): weight Ã—500â€“800
- Historical (<2000): weight Ã—50

**Regime Router:**
```python
class RegimeRouter:
    def __init__(self):
        self.regime_detector = LightGBMClassifier(
            n_estimators=100, max_depth=5, random_state=42
        )
        self.regime_models = {}

    def predict(self, features):
        regime = self.regime_detector.predict(features)[0]
        return self.regime_models[regime].predict(features)
```

**Regime-Specific Models:**
- Crisis regime: 2-layer LSTM + GRU (most important for risk)
- Bull/Bear/Normal: 1 architecture each
- Count: 8-10 regime models total

### Phase 3: Horizon-Specific Optimization

**ZL Horizons** (5 total - Daily forecast horizons only):
- 1 week, 1 month, 3 months, 6 months, 12 months
- Model count: 30-35 models (baselines + regime + advanced)
- Features: Daily features, focus on fundamentals

- **Intraday (minutes):** 1 min, 5 min, 15 min, 30 min
  - Neural nets (LSTM, TCN, CNN-LSTM)
  - Features: 150-200 (microstructure, orderflow, volume)
- **Intraday (hours):** 1 hour, 4 hours
  - Neural nets (LSTM, TCN, CNN-LSTM)
  - Features: 150-200 (microstructure, orderflow, volume)
- **Multi-day:** 1 day, 7 days, 30 days
  - Tree models (LightGBM, XGBoost)
  - Features: 100-150 (macro, sentiment, positioning)
- **Multi-month:** 3 months, 6 months, 12 months
  - Tree models (LightGBM, XGBoost)
  - Features: 100-150 (macro, sentiment, positioning)
- Model count: 35-40 models

**For Each Horizon:**
- Compare all baselines on holdout data
- Select top performer
- Add horizon-specific features
- Fine-tune architecture

### Phase 4: Advanced Neural & Ensemble

**Advanced Neural Architectures:**
- 2-layer LSTM (2-3 variants, units 64-128)
- 2-layer GRU (2-3 variants)
- TCN (1-2 variants, kernel_size 3-5)
- CNN-LSTM hybrid (1-2 variants)
- Optional: 1-2 TINY attention (heads â‰¤4, d_model â‰¤256, seq_len â‰¤256)

**Memory Constraints:**
- FP16 mixed precision (MANDATORY for 16GB RAM)
- Batch sizes: LSTM â‰¤32, TCN â‰¤32, attention â‰¤16
- Sequential training (one GPU job at a time)
- Gradient checkpointing and memory cleanup
- External SSD for all checkpoints/logs

**Ensemble & Meta-Learning:**
- Regime-aware weighting (crisis = weight 1W more, calm = weight 6M more)
- LightGBM meta-learner on OOF predictions + regime context
- Weighted averaging with dynamic regime-aware weights
- Uncertainty quantification (MAPIE conformal prediction for 90% confidence bands)

**Production Optimization:**
- Quantize neural models via TFLite FP16
- Convert tree models to ONNX
- `ProductionPredictor` class handles scaling, feature selection, regime routing, monotonicity checks, and bounds enforcement (Â±30% annualized)

### Memory & Scheduling Controls

**Hardware Constraints:**
- Mac M4: 16GB unified memory + TensorFlow Metal GPU
- Environment: `vertex-metal-312` (Python 3.12.6)
- Sequential training: One GPU job at a time (prevent thermal throttling)

**Memory Management:**
```python
# Aggressive cleanup between models
import gc
import tensorflow as tf

def cleanup_session():
    gc.collect()
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()

# Forced cooldown between neural models
import time
time.sleep(60)  # Prevent thermal throttling
```

**Feature Counts by Phase:**
- Phase 1 (Baselines): 400 features (full feature set)
- Phase 2 (Regime): 60-80 features (regime-specific)
- Phase 3 (Horizon): 40-60 features (horizon-optimized)
- Phase 4 (Neural): 30-40 features (temporal windows)

**Training Schedule:**
- Sequential execution (not parallel)
- Clear Keras sessions after each model
- Monitor Activity Monitor for memory pressure
- Use `model.partial_fit` for large datasets

### Summary

- **Total models:** 65-75 total
  - ZL: 30-35 models (5 horizons)
- **Training time:** ~22 hours spread across execution plan (Day 1-7 intensive, then ongoing)
- **Expected uplift:** Phase 1 â†’ Phase 4 yields 60-85% accuracy improvement, with regime-specific + neural phases contributing incremental gains before the final ensemble.

**See:** `docs/plans/TRAINING_MASTER_EXECUTION_PLAN.md` for detailed day-by-day execution plan (Day 1-7) with specific scripts, memory management, and production deployment steps.

---

## SUPER-OPTIMIZED API-FACING OVERLAY LAYER

**Purpose:** Curated views that combine raw tables into dashboard-ready, single-query endpoints. These views eliminate complex joins at query time and provide consistent interfaces for the Vercel dashboard and training exports.

### Overlay Views (Pivot-Only)

- Overlays must read from pre-pivoted wide tables (or the daily ML matrix) and avoid multi-way joins.
- API overlays: select and relabel fields from pivoted features/predictions tables; avoid re-joining macro/regime tablesâ€”precompute in Dataform.
- Predictions overlays: simple passthrough with metadata (model version, confidence) sourced from the predictions table itself.
- Regime overlays: precompute regime/context into a single wide table; downstream views are projections only.

### Big 8 Refresh Job (Pivoted)

- `scripts/sync/sync_signals_big8.py` recalculates `signals.big_eight_live` every 15 minutes from pivoted source tables; result is a single wide row per timestamp (no joins in docs).
- Inputs: crush, energy proxies, policy events, hidden relationships, volatility, positioning, FX, weatherâ€”each already prefixed and pivoted by date.
- Output: Big 8 composite + components + narrative fields.

### View Creation Order (No Join SQL in Docs)

1. Create base tables (schema SQL) via Dataform definitions.
2. Create compatibility passthrough views for training tables.
3. Create signals composites from pivoted tables (Big 8).
5. Create regime overlay from precomputed regime table.
6. Create API/prediction overlays as projections (no joins).

---

## DASHBOARD ARCHITECTURE (5 PAGES - FINAL)

**Status:** Corrected November 19, 2025  
**Reference:** `docs/PAGE_BUILDOUT_ROADMAP.md` for complete specifications

### Final Architecture - 5 Pages Total

1. **Dashboard** â€“ Chris's Daily Money Screen
   - Daily decisions (morning screen)
   - Full-width 1-month chart with SHAP overlays
   - 4 horizon charts (1w, 3m, 6m, 12m)
   - Procurement traffic light + briefing
   - Live SHAP beeswarm + alpha tracker
   - **Data Sources:** `predictions.vw_zl_latest`, `models_v4.shap_daily`, `market_data.databento_futures_ohlcv_1d`, `raw_intelligence.sentiment_daily`

2. **Sentiment** â€“ Market Mood (9-Layer Engine)
   - Production-ready 9-layer sentiment system
   - Procurement Sentiment Index
   - Pinball triggers
   - **Reference:** `docs/reference/SENTIMENT_ARCHITECTURE_REFERENCE.md`

3. **Strategy** â€“ Chris's What-If Playground
   - Business intelligence / what-if war room
   - 6 scenario cards (NOT sliders - interactive cards with click-to-adjust):
     - Volatility Card: VIX Â± 1â€“2Ïƒ (widens/narrows quantile bands)
     - Biofuel Demand Card: EIA biodiesel + RIN D4 Î”%
     - FX Card: BRL Â±X%, CNY Â±X% (export competitiveness)
     - Trade Card: China weekly export sales shock (percentile toggle)
     - Weather Card: ONI phase toggle + Brazil/Argentina regional tilts
     - Substitutes Card: Palm oil +/âˆ’10% (elasticity link)
   - Instant 5,000-path Monte Carlo re-run
   - SHAP overlays on all charts
   - Partial derivatives table
   - Walk-forward validation
   - **Language:** Procurement-focused (NO "hedge" or "hedging" - Chris is procuring, not trading)
   - **Data Sources:** `raw_intelligence.sentiment_daily`, `predictions.vw_zl_latest`, `models_v4.shap_daily`
   - **Reference:** `docs/reference/STRATEGY_SCENARIO_CARDS.md` for complete scenario card specifications

4. **Trade Intelligence** â€“ Geopolitical Nuke Room
   - Geopolitical risk ("how the fuck did you know that" page)
   - Focus: Trump, EPA, Argentina, China, Venezuela, lobbying, tariffs
   - Force-directed graph (D3/Recharts) with hidden connections
   - Neural net for non-obvious links
   - SHAP overlays on historical timeline
   - **Data Sources:** 
     - `raw_intelligence.news_articles` (latest biofuel/EPA/Argentina/China/Venezuela)
     - `policy_trump_signals` (tariff scores)
     - `features.master_features` (neural embeddings for dot-connecting)
     - **`zl_impact_predictor.py` output** (`dashboard-nextjs/public/api/zl_impact.json`) - This is the CORRECT source for "ZL impact" on this page (Trump action impacts on procurement)
   - **CRITICAL:** This is the ONLY page that should use `zl_impact_predictor.py` output. Vegas Intel uses `predictions.vw_zl_latest` instead.

5. **Vegas Intel** â€“ Kevin-Only Upsell Machine
   - Restaurant upsell weapon (Glide connection)
   - Interactive Vegas map with volume multipliers
   - ZL forecast integration (from `predictions.vw_zl_latest` - NOT `zl_impact_predictor.py`)
   - One-click proposals
   - **Data Sources:** 
     - **Glide API** (live data: restaurants served, oil delivered, fryer capacities, volume multipliers)
     - `predictions.vw_zl_latest` (ZL forecast - NOT zl_impact, which is for Trump actions)
     - Internal restaurant accounts
   - **CRITICAL:** Use `predictions.vw_zl_latest` for ZL price forecasts. Do NOT use `zl_impact_predictor.py` output here - that's specifically for Trump action impacts on Trade Intelligence page.

### Hidden Page (Not Part of Main Dashboard)

  - **Separate from main dashboard**
  - **NOT connected to Vegas Intel**

### Critical Rules

- **5 PAGES TOTAL** - No more, no less
- **Trade Intelligence** (not "Legislative") - Focus on geopolitical risk
- **NO status report headers** - Remove any "What We're Doing" boilerplate
- **SHAP overlays required** on Dashboard, Strategy, and Trade Intelligence charts

### SHAP Overlay Requirements

**Component:** `components/ShapOverlay.tsx` (reusable)

**Implementation:**
- Second Y-axis (right side) scaled -2 to +2 (SHAP impact in cents/lb)
- 4 glowing force lines (top 4 drivers):
  1. RINs momentum (orange)
  2. Geopolitical tariff score (red when negative)
  3. South America weather anomaly (blue)
  4. Crush margin proxy (green)
- Each line: `strokeWidth 3`, `dot={false}`, `animationDuration 800ms`
- Tooltip: "RINs +180% â†’ +11.2Â¢ contribution to 1mo forecast"
- Floating legend: "Today's Top Drivers â†’ +18.4Â¢ total lift"
- Bold + pulsing glow when driver is #1 contributor

**Data Source:** `cbi-v15.models_v4.shap_daily` (date, horizon, feature_name, shap_value_cents)

**Pages Requiring SHAP:**
- Dashboard: All 5 charts (1M main + 4 horizon minis)
- Strategy: All Monte Carlo cone charts (update on slider move)
- Trade Intelligence: Historical timeline chart (30-day SHAP after each event)

---

## DASHBOARD EXECUTION BACKLOG (Post-Data Phase)

These cards capture how Chris consumes intelligence. Build them once staging + modeling are live so the dashboard answers his five core questions (â€œWhat moved?â€, â€œIs this normal?â€, â€œWhat do I do?â€, â€œHow does it compare?â€, â€œIs risk rising?â€).

1. **Inline Explainables**
   - Top 3 SHAP drivers per forecast with plain-English text (â€œPalm spread widening + BRL weakness drove +0.8% biasâ€).
   - Macro/news injectors (USDA flashes, palm export chatter, Fed signals) rendered as a one-sentence causal chain.

2. **Regime Indicator Panel**
   - Compact widget: regime name, days active, historical impact profile, and confidence (â€œVolatility-High | Active since Octâ€¯12 | soy oil reacts 1.4Ã— to palm shocksâ€).

3. **â€œWhat Changed Since Yesterdayâ€ Tile**
   - Delta list (palm +1.8%, USD/BRL âˆ’0.7%, crush +0.3%, weather +12â€¯% rain, funds +4k) with net effect classification (+0.3Ïƒ upward pressure).

4. **Substitution Economics Analyzer**
   - Live palm/ZL spread, percentile rank, spread regime (tight/normal/wide), historical analog callouts, and mean-reversion vs breakout probability.

5. **Scenario Buttons (â€œIf thisâ†’then thatâ€)**
   - Palm +5%, BRL +3%, crude âˆ’8%, Brazil drought, Indo quota cut etc. Buttons recalc fair value shift, forecast shift, risk status, explainer snippet.

6. **Confidence & Signal Reliability Meter**
   - Confidence score, historical hit rate for current driver combo, volatility drag, ensemble distribution width, and a â€œWhy lowered/raised?â€ caption.

7. **Positioning & Flow Intelligence**
   - Market pressure gauge combining CFTC fund deltas, crush rate change, export velocity, front/back roll, short vs long pressure.

8. **Historical Analog Finder**
   - Auto text: â€œLast 5 times (Vol-High + Palm Up + BRL Weak) â†’ ZL rose 4/5 with avg +2.1% over 20 days.â€ Confidence boost module.

9. **Procurement Timing Heat Map**
   - Horizon table (1w/1m/3m/6m) with signal, confidence, recommended action (HOLD/SMALL BUY/WAIT/BUY HEAVY) color-coded.

10. **Risk Alerts & Threshold Alarms**
    - Alert engine for palm surge >3â€¯%, BRL vol spike, Indo policy chatter, crush collapse, weather anomaly, regime switch, VIX >30. Each alert explains consequence (e.g., â€œPalm +3.9â€¯% â†’ substitution pressure â†‘ next 5â€“15â€¯dâ€).

11. **Narrative Strip**
    - 1â€“2 sentence auto summary of todayâ€™s drivers, shifts, forecast direction, and risk tone (â€œMild bullish window: palm strength + weaker BRL vs soft crude; confidence moderate.â€).

12. **Procurement Value at Risk (Pâ€‘VaR)**
    - Quantifies dollar impact of waiting vs buying now: expected return, vol, position size, risk window â†’ cost avoidance, worst-case slippage, probability-weighted outcomes.

13. **Model Drift Panel**
    - Change log for feature ranks, regime weights, prediction variance vs prior run. Explains â€œWhy did the model change?â€ for trust.

> When ready to implement, spin up a dedicated dashboard spec covering wireframes, color system, BigQuery views/API endpoints. Use this backlog as the checklist.

---

**Last Updated:** November 17, 2025  
**Status:** Ready for Execution  
**Next Step:** Delete legacy files, start fresh build
